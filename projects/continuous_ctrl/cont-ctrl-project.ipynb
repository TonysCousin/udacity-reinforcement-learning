{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control Project\n",
    "\n",
    "I'll begin this project by implementing a straight DDPG solution and see how that works.  Depending on the amount of time it takes, I hope to follow up with additional layers of sophistication.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1 - Simple DDPG\n",
    "\n",
    "The code below is based on the Udacity instructor-provided code from the financial markets lesson.  I have modified it somewhat to adapt to the robot arm problem for this project.\n",
    "\n",
    "### Set up the Unity reacher environment for a single arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python\n",
    "print(\"Python install done.\")\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env = UnityEnvironment(file_name='Reacher_Linux/Reacher.x86_64')\n",
    "print(\"Environment defined.\")\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Set up the environment\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from ddpg_agent import DdpgAgent\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINNING_SCORE = 30.0 #average over 100 consecutive episodes\n",
    "\n",
    "def train(agent, env, run_name=\"UNDEF\", max_episodes=20, max_time_steps=1000, break_in=512, sleeping=False):\n",
    "\n",
    "    # Initialize simulation environment\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    #env_info = env.reset(train_mode=True)[brain_name]\n",
    "    #state_size = len(env_info.vector_observations[0])\n",
    "    #action_size = brain.vector_action_space_size\n",
    "\n",
    "    scores = []\n",
    "    recent_scores = deque(maxlen=100)\n",
    "    start_time = time.perf_counter()\n",
    "    starting_point = 0\n",
    "\n",
    "    # loop on episodes\n",
    "    for e in range(starting_point, max_episodes):\n",
    "        \n",
    "        # Reset the enviroment\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0 #total score for this episode\n",
    "\n",
    "        # loop over time steps\n",
    "        for i in range(max_time_steps):\n",
    "\n",
    "            # Predict the best action for the current state.\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # get the new state & reward based on this action\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "\n",
    "            # update the agent with this new info\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            # roll over new state\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # determine epoch duration and estimate remaining time\n",
    "        current_time = time.perf_counter() #was time.clock()\n",
    "        avg_duration = (current_time - start_time) / (e - starting_point + 1) / 60.0 #minutes\n",
    "        remaining_time_minutes = (starting_point + max_episodes - e - 1) * avg_duration\n",
    "        rem_time = remaining_time_minutes / 60.0\n",
    "        time_est_msg = \"{:4.1f} hr rem.\".format(rem_time)\n",
    "\n",
    "        # update score bookkeeping, report status and decide if training is complete\n",
    "        scores.append(score)\n",
    "        recent_scores.append(score)\n",
    "        avg_score = np.mean(recent_scores)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}, avg {:.1f} episodes/min'\n",
    "              .format(e, avg_score, 1.0/avg_duration), end=\"\")\n",
    "        if e > 0  and  e % 50 == 0:\n",
    "            torch.save(agent.actor_local.state_dict(),  '{}_checkpointa_{:d}.pt'.format(run_name, e))\n",
    "            torch.save(agent.critic_local.state_dict(), '{}_checkpointc_{:d}.pt'.format(run_name, e))\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\t{}             '.format(e, avg_score, time_est_msg))\n",
    "\n",
    "        if sleeping:\n",
    "            if e % 50 < 5:\n",
    "                time.sleep(1) #allow time to view the Unity window\n",
    "\n",
    "        if e > 100  and  avg_score >= WINNING_SCORE:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, avg_score))\n",
    "            torch.save(agent.actor_local.state_dict(),  '{}_checkpointa.pt'.format(run_name))\n",
    "            torch.save(agent.critic_local.state_dict(), '{}_checkpointc.pt'.format(run_name))\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Train the model & observe its progress\n",
    "\n",
    "RUN_NAME = 'D'\n",
    "BATCH = 128\n",
    "DECAY = 0.99999\n",
    "LEARN_EVERY = 25\n",
    "LEARN_ITER  = 40\n",
    "a = DdpgAgent(33, 4, random_seed=0, batch_size=BATCH, noise_decay=DECAY, \n",
    "              learn_every=LEARN_EVERY, learn_iter=LEARN_ITER)\n",
    "\n",
    "scores = train(a, env, run_name=RUN_NAME, max_episodes=1000, max_time_steps=1000, \n",
    "               break_in=BATCH, sleeping=True)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a pre-trained agent\n",
    "\n",
    "Use the code below to load an agent from checkpoint files and run it in inference mode.\n",
    "\n",
    "**Note:**  It uses an environment and some other variables set up above, so don't close the environment in the previous cell, and don't plan to run this cell by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the names of the two checkpoint files you want to run here:\n",
    "ACTOR =  \"B/checkpointa_950.pt\"\n",
    "#CRITIC = \"B/checkpointc_100.pt\"\n",
    "\n",
    "#BATCH = 128\n",
    "#DECAY = 0.99999\n",
    "#LEARN_EVERY = 25\n",
    "#LEARN_ITER  = 40\n",
    "\n",
    "inference_agent = DdpgAgent(33, 4, random_seed=19004, actor_file=ACTOR)\n",
    "\n",
    "for episode in range(3):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]   \n",
    "    state = env_info.vector_observations[0]\n",
    "    score = 0.0\n",
    "    print(\"Episode {} \".format(episode), end='')\n",
    "    \n",
    "    for time_step in range(500):\n",
    "        action = inference_agent.act(state)\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        inference_agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            print(\"Done signal.\")\n",
    "            break\n",
    "            \n",
    "    print(\"total score: {:.2f}\".format(score))\n",
    "    time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
