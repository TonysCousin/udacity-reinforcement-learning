{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs John's solution for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.  It uses the Unity ML-Agents environment to train two cooperative agents to play a tennis-like game.\n",
    "\n",
    "See the project [README](README.md) for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code depends upon a custom Unity environment provided by the Udacity staff that embodies the variation on tennis.  It will open a separate Unity window for visualizing the environment as the agents train or play.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how we will use this notebook - JOHN FIX THIS!!!!\n",
    "\n",
    "In the next cell, set the appropriate values of a couple control variables:\n",
    "- **EXPLORE** determines whether the notebook does exploratory training or inference demonstration.\n",
    "    - **True** runs a hyperparameter exploration loop to generate many training runs with a random search algorithm.  To use this well, you should study that cell and specify the ranges of hyperparameters to be explored.\n",
    "    - **False** runs a few inference episodes of a pretrained model and opens a visualization window to watch it play.\n",
    "- **config_name:** the name of a model configuration to be loaded from a checkpoint to begin the exercise.  \n",
    "    - If EXPLORE = True, this is optional, and tells the training loop to start from this pre-trained model and continue refining it; if the value is _None_ then the training starts from a randomly initialized model.\n",
    "    - If EXPLORE = False, then this must reflect the name of a legitimate config/run (e.g. \"M37.01\").\n",
    "- **checkpoint_episode:** if a checkpoint is being used to start the exercise, then this number reflects what episode that checkpoint was captured from.  The checkpoint_name and checkpoint_episode together are required to completely identify the checkpoint file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORE            = False\n",
    "config_name        = \"M46X\" # Must be None if not using!\n",
    "run_number         = 1\n",
    "checkpoint_episode = 5005\n",
    "training_viz       = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "from train import train\n",
    "from maddpg import Maddpg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "initial_episode = checkpoint_episode\n",
    "checkpoint_path = \"checkpoint/{}/\".format(config_name)\n",
    "tag = \"{}.{:02d}\".format(config_name, run_number)\n",
    "\n",
    "if EXPLORE:\n",
    "    turn_off_graphics = not training_viz\n",
    "    initial_episode = 0\n",
    "    unity_train_mode = True\n",
    "    if config_name != None:\n",
    "        initial_episode = checkpoint_episode\n",
    "else:\n",
    "    turn_off_graphics = False\n",
    "    unity_train_mode = False\n",
    "\n",
    "# create a new Unity environment\n",
    "# it needs to be done once, outside any loop, as closing an environment then restarting causes\n",
    "# a Unity exception about the handle no longer being active.\n",
    "env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\", seed=0, \n",
    "                       no_graphics=turn_off_graphics)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]                       \n",
    "env_info = env.reset(train_mode=unity_train_mode)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agents\n",
    "\n",
    "The next cells will invoke the training program to create the agents.  All of the real code is in Python flat files in this project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RandomSampler():\n",
    "    \"\"\"Serves up random combinations of hyperparameters from defined sets or ranges of values to explore.\"\"\"\n",
    "    \n",
    "    def __init__(self, vars):\n",
    "        \"\"\"Accepts definition of the set of variables to be sampled.\n",
    "            \n",
    "            Params:\n",
    "                vars (list of lists): each item is a list containing:\n",
    "                                        item 0 - either 'discrete', 'continuous-int' or 'continuous-float'\n",
    "                                        items 1-N depend on the value of item 0:\n",
    "                                        if discrete, then these are the set of values to be chosen from\n",
    "                                        if continuous then these are the min & max bounds of the range\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vars = vars\n",
    "        \n",
    "        self.rng = np.random.default_rng()\n",
    "    \n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Draws a random sample (uniform distribution) of all variables at its disposal.\n",
    "        \n",
    "            Returns a list of values in the order of definition.\n",
    "        \"\"\"\n",
    "\n",
    "        rtn = []\n",
    "        for v in self.vars:\n",
    "            if v[0] == \"discrete\":\n",
    "                choice = self.rng.integers(low=1, high=len(v), size=1)[0]\n",
    "                rtn.append(v[choice])\n",
    "                \n",
    "            elif v[0] == \"continuous-int\":\n",
    "                choice = self.rng.integers(low=v[1], high=v[2], size=1)[0]\n",
    "                rtn.append(choice)\n",
    "                \n",
    "            elif v[0] == \"continuous-float\":\n",
    "                choice = self.rng.random() * (v[2]-v[1]) + v[1]\n",
    "                rtn.append(choice)\n",
    "            \n",
    "            else:\n",
    "                print(\"///// RandomSampler error:  unknown type \", v[0])\n",
    "            \n",
    "        return rtn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell will explore several combinations of hyperparams by training all of them\n",
    "# Use a random search for the hyperparams\n",
    "\n",
    "TIME_STEPS         = 600\n",
    "\n",
    "if EXPLORE:\n",
    "    \n",
    "    # fixed for the session:\n",
    "    RUN_PREFIX        = \"M46X\" #extended from M46\n",
    "    EPISODES          = 10001\n",
    "    NUM_RUNS          = 6\n",
    "    BUFFER_PRIME_SIZE = 50\n",
    "    WEIGHT_DECAY      = 1.0e-5 #was 1.0e-5\n",
    "    GAMMA             = 0.99\n",
    "    LR_ANNEAL_FREQ    = 10000 #episodes\n",
    "    LR_ANNEAL_MULT    = 1.0\n",
    "    WIN_SCORE         = 1.0\n",
    "    SEED              = 44939 #(0, 111, 468, 5555, 23100, 44939)\n",
    "    \n",
    "    # session variables:\n",
    "    vars = [\n",
    "            [\"discrete\",               1.00],      #BAD_STEP_PROB\n",
    "            [\"continuous-float\", 0.999000,  0.999900],  #NOISE_DECAY\n",
    "            [\"discrete\",         0.010],                #NOISE_SCALE (was 0.040, 1.0)\n",
    "            [\"continuous-float\", 0.000800,  0.000900],  #LR_ACTOR  (was 0.000010, 0.000080)\n",
    "            [\"continuous-float\", 0.1,      0.3],       #LR_RATIO (determines LR_CRITIC)\n",
    "            [\"discrete\",         2       ],             #LEARN_EVERY\n",
    "            [\"continuous-int\",   1,         2],         #LEARN_ITER\n",
    "            [\"continuous-float\", 0.00100,   0.01000],   #TAU\n",
    "            [\"discrete\",         256]                   #BATCH\n",
    "           ]\n",
    "    rs = RandomSampler(vars)\n",
    "    \n",
    "    print(\"Ready to train {} over {} training sets for {} episodes each, with fixed params:\"\n",
    "          .format(RUN_PREFIX, NUM_RUNS, EPISODES))\n",
    "    print(\"    Max episodes   = \", EPISODES)\n",
    "    print(\"    Weight decay   = \", WEIGHT_DECAY)\n",
    "    print(\"    Gamma          = \", GAMMA)\n",
    "    print(\"    LR anneal freq = \", LR_ANNEAL_FREQ)\n",
    "    print(\"    LR anneal mult = \", LR_ANNEAL_MULT)\n",
    "    print(\"    Buf prime size = \", BUFFER_PRIME_SIZE)\n",
    "            \n",
    "    for set_id in range(NUM_RUNS):\n",
    "        \n",
    "        # sample the variables\n",
    "        v = rs.sample()\n",
    "        BAD_STEP_PROB = v[0]\n",
    "        NOISE_DECAY   = v[1]\n",
    "        NOISE_SCALE   = v[2]\n",
    "        LR_ACTOR      = v[3]\n",
    "        LR_CRITIC     = v[4] * LR_ACTOR\n",
    "        LEARN_EVERY   = v[5]\n",
    "        LEARN_ITER    = v[6]\n",
    "        TAU           = v[7]\n",
    "        BATCH         = v[8]\n",
    "\n",
    "        buffer_size = 100000\n",
    "\n",
    "        RUN_NAME = \"{}.{:02d}\".format(RUN_PREFIX, set_id)\n",
    "        print(\"\\n///// Beginning training set \", RUN_NAME, \" with:\")\n",
    "        print(\"      Batch size       = {:d}\".format(BATCH))\n",
    "        print(\"      Buffer size      = {:d}\".format(buffer_size))\n",
    "        print(\"      Bad step prob    = {:.4f}\".format(BAD_STEP_PROB))\n",
    "        print(\"      Noise decay      = {:.6f}\".format(NOISE_DECAY))\n",
    "        print(\"      Noise scale      = {:.3f}\".format(NOISE_SCALE))\n",
    "        print(\"      LR actor         = {:.7f}\".format(LR_ACTOR))\n",
    "        print(\"      LR critic        = {:.7f}\".format(LR_CRITIC))\n",
    "        print(\"      Learning every     \", LEARN_EVERY, \" time steps\")\n",
    "        print(\"      Learn iterations = \", LEARN_ITER)\n",
    "        print(\"      Tau              = {:.5f}\".format(TAU))\n",
    "        print(\"      Seed             = \", SEED)\n",
    "\n",
    "        ##### instantiate the agents and perform the training\n",
    "\n",
    "        maddpg = Maddpg(state_size, action_size, 2, bad_step_prob=BAD_STEP_PROB,\n",
    "                        random_seed=SEED, batch_size=BATCH, buffer_size=buffer_size,\n",
    "                        noise_decay=NOISE_DECAY, buffer_prime_size=BUFFER_PRIME_SIZE,\n",
    "                        learn_every=LEARN_EVERY, \n",
    "                        learn_iter=LEARN_ITER, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC,\n",
    "                        lr_anneal_freq=LR_ANNEAL_FREQ, lr_anneal_mult=LR_ANNEAL_MULT,\n",
    "                        weight_decay=WEIGHT_DECAY, gamma=GAMMA, noise_scale=NOISE_SCALE,\n",
    "                        tau=TAU)\n",
    "        \n",
    "        if config_name != None:\n",
    "            print(\"///// Beginning training from checkpoint for {}, episode {}\" \\\n",
    "                  .format(tag, initial_episode))\n",
    "            maddpg.restore_checkpoint(checkpoint_path, tag, initial_episode)\n",
    "\n",
    "        scores, avgs = train(maddpg, env, run_name=RUN_NAME, starting_episode=initial_episode,\n",
    "                             max_episodes=EPISODES, sleeping=training_viz, winning_score=WIN_SCORE,\n",
    "                             max_time_steps=TIME_STEPS, checkpoint_interval=1000)\n",
    "\n",
    "        ##### plot the training reward history\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(scores)), scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(avgs)), avgs)\n",
    "        plt.ylabel('Avg Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "\n",
    "                            \n",
    "    print(\"\\n\\nDONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEY JOHN - TODO!\n",
    "\n",
    "- update main.py to match the above code {ALL CELLS}\n",
    "- Test running from cmd line (may need a script?)\n",
    "- Clean up the bottom part of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run two trained agents against each other (inference mode)\n",
    "\n",
    "Note:  before running this cell, the Unity environment object will need to be defined (at top of notebook) with `no_graphics=False` so that the graphical game display will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint v4 loaded for M46X.01, episode 5005\n",
      "Episode 0: 0.190, took 64 steps\n",
      "Episode 1: 0.190, took 65 steps\n",
      "Episode 2: 0.000, took 13 steps\n",
      "Episode 3: 0.800, took 315 steps\n",
      "Episode 4: 0.100, took 66 steps\n",
      "Episode 5: 0.200, took 67 steps\n",
      "Episode 6: 0.200, took 67 steps\n",
      "Episode 7: 0.100, took 29 steps\n",
      "Episode 8: 0.190, took 64 steps\n",
      "Episode 9: 0.290, took 102 steps\n",
      "Episode 10: 2.600, took 1001 steps\n",
      "Episode 11: 2.200, took 836 steps\n",
      "Episode 12: 0.800, took 296 steps\n",
      "Episode 13: 0.300, took 106 steps\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if not EXPLORE:\n",
    "    \n",
    "    # load the pre-trained model\n",
    "    model = Maddpg(state_size, action_size, 2)\n",
    "    model.restore_checkpoint(checkpoint_path, tag, initial_episode)\n",
    "\n",
    "    for i in range(14):                                        # play game for several episodes\n",
    "        env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        num_steps = 0\n",
    "        while True:\n",
    "            actions = model.act(states, is_inference=True, add_noise=False)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            num_steps += 1\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                time.sleep(2)\n",
    "                break\n",
    "        print('Episode {}: {:5.3f}, took {} steps'.format(i, np.max(scores), num_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd-p2",
   "language": "python",
   "name": "p2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
