{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs John's solution for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.  It uses the Unity ML-Agents environment to train two cooperative agents to play a tennis-like game.\n",
    "\n",
    "**Need more description here - refer to readme?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code depends upon a custom Unity environment provided by the Udacity staff that embodies the variation on tennis.  It will open a separate Unity window for visualizing the environment as the agents train or play.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how we will use this notebook - JOHN FIX THIS!!!!\n",
    "\n",
    "In the next cell, set the appropriate values of a couple control variables:\n",
    "- **EXPLORE** determines whether the notebook does exploratory training or inference demonstration.\n",
    "    - **True** runs a hyperparameter exploration loop to generate many training runs with a random search algorithm.  To use this well, you should study that cell and specify the ranges of hyperparameters to be explored.\n",
    "    - **False** runs a few inference episodes of a pretrained model and opens a visualization window to watch it play.\n",
    "- **config_name:** the name of a model configuration & run to be loaded from a checkpoint to begin the exercise.  \n",
    "    - If EXPLORE = True, this is optional, and tells the training loop to start from this pre-trained model and continue refining it; if the value is _None_ then the training starts from a randomly initialized model.\n",
    "    - If EXPLORE = False, then this must reflect the name of a legitimate config/run (e.g. \"M37.01\").\n",
    "- **checkpoint_episode:** if a checkpoint is being used to start the exercise, then this number reflects what episode that checkpoint was captured from.  The checkpoint_name and checkpoint_episode together are required to completely identify the checkpoint file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORE            = True\n",
    "config_name        = \"M37\" # Must be None if not using!\n",
    "run_number         = 29\n",
    "checkpoint_episode = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "from train import train\n",
    "from maddpg import Maddpg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "initial_episode = checkpoint_episode\n",
    "checkpoint_path = \"checkpoint/{}/\".format(config_name)\n",
    "tag = \"{}.{:02d}\".format(config_name, run_number)\n",
    "\n",
    "if EXPLORE:\n",
    "    turn_off_graphics = True\n",
    "    initial_episode = 0\n",
    "    if config_name != None:\n",
    "        initial_episode = checkpoint_episode\n",
    "else:\n",
    "    turn_off_graphics = False\n",
    "\n",
    "# create a new Unity environment\n",
    "# it needs to be done once, outside any loop, as closing an environment then restarting causes\n",
    "# a Unity exception about the handle no longer being active.\n",
    "env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\", seed=0, \n",
    "                       no_graphics=turn_off_graphics)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]                       \n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agents\n",
    "\n",
    "The next cells will invoke the training program to create the agents.  All of the real code is in Python flat files in this project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 \n",
      " [99, 500, 43, 7, 0.7838081490361567, -2.2143198774495287, 2.7836519036225593]\n",
      "\n",
      " 1 \n",
      " [99, 500, 43, 3, 0.30628932239099704, -1.306612947303285, 4.5957475558402905]\n",
      "\n",
      " 2 \n",
      " [88, 500, 43, 8, 0.5061336553935076, -1.2164617531073345, 1.534579393998198]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RandomSampler():\n",
    "    \n",
    "    def __init__(self, vars):\n",
    "        \"\"\"Accepts definition of the set of variables to be sampled.\n",
    "            \n",
    "            Params:\n",
    "                vars (list of lists): each item is a list containing:\n",
    "                                        item 0 - either 'discrete', 'continuous-int' or 'continuous-float'\n",
    "                                        items 1-N depend on the value of item 0:\n",
    "                                        if discrete, then these are the set of values to be chosen from\n",
    "                                        if continuous then these are the min & max bounds of the range\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vars = vars\n",
    "        \n",
    "        self.rng = np.random.default_rng()\n",
    "    \n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Draws a random sample of all variables at its disposal.\n",
    "        \n",
    "            Returns a list of values in the order of definition.\n",
    "        \"\"\"\n",
    "\n",
    "        rtn = []\n",
    "        for v in self.vars:\n",
    "            if v[0] == \"discrete\":\n",
    "                choice = self.rng.integers(low=1, high=len(v), size=1)[0]\n",
    "                rtn.append(v[choice])\n",
    "                \n",
    "            elif v[0] == \"continuous-int\":\n",
    "                choice = self.rng.integers(low=v[1], high=v[2], size=1)[0]\n",
    "                rtn.append(choice)\n",
    "                \n",
    "            elif v[0] == \"continuous-float\":\n",
    "                choice = self.rng.random() * (v[2]-v[1]) + v[1]\n",
    "                rtn.append(choice)\n",
    "            \n",
    "            else:\n",
    "                print(\"///// RandomSampler error:  unknown type \", v[0])\n",
    "            \n",
    "        return rtn\n",
    "                \n",
    "vars = [[\"discrete\", 88, 66, 11, 22, 33, 44, 99, 101, 77],\n",
    "        [\"discrete\", 500], #1-item list\n",
    "        [\"continuous-int\", 43, 44], #1-item range\n",
    "        [\"continuous-int\", 0, 10],\n",
    "        [\"continuous-float\", 0.0, 1.0],\n",
    "        [\"continuous-float\", -3.3, 0.0],\n",
    "        [\"continuous-float\", -1.0, 6.4],\n",
    "       ]\n",
    "rs = RandomSampler(vars)\n",
    "\n",
    "for i in range(3):\n",
    "    out = rs.sample()\n",
    "    print(\"\\n\", i, \"\\n\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to train M38 over 40 training sets for 15001 episodes each, with fixed params:\n",
      "    Max episodes   =  15001\n",
      "    Weight decay   =  0.0\n",
      "    Gamma          =  0.99\n",
      "    LR anneal freq =  2000\n",
      "    LR anneal mult =  0.8\n",
      "    Buf prime size =  5000\n",
      "\n",
      "///// Beginning training set  M38.00  with:\n",
      "      Batch size       = 128\n",
      "      Buffer size      = 100000\n",
      "      Bad step prob    = 0.1000\n",
      "      Noise decay      = 0.999995\n",
      "      Noise scale      = 0.041\n",
      "      LR actor         = 0.0009006\n",
      "      LR critic        = 0.0009919\n",
      "      Learning every      70  time steps\n",
      "      Learn iterations =  2\n",
      "      Tau              = 0.00118\n",
      "      Seed             =  44939\n",
      "///// Beginning training from checkpoint for M37.29, episode 2000\n",
      "Checkpoint loaded for M37.29, episode 2000\n",
      "Priming the replay buffer................................................\n",
      "\n",
      "2000\tAverage score:   0.000,        mem:   5003/     0 ( 0.0%), avg 2255.2 eps/min;  0.1 hr rem   \n",
      "2519\tRunning avg/max: 0.000/0.000,  mem:   5739/     0 ( 0.0%), avg 1195.8 eps/min   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4a9602ec59de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m         scores, avgs = train(maddpg, env, run_name=RUN_NAME, starting_episode=initial_episode,\n\u001b[1;32m     95\u001b[0m                              \u001b[0mmax_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinning_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTIME_STEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                              checkpoint_interval=2000)\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m##### plot the training reward history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Udacity/ReinforcementLearning/repo/projects/tennis/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(maddpg, env, run_name, starting_episode, max_episodes, max_time_steps, sleeping, winning_score, checkpoint_interval)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# advance the MADDPG model and its environment by one time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m               \u001b[0madvance_time_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaddpg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# check for episode completion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Udacity/ReinforcementLearning/repo/projects/tennis/train.py\u001b[0m in \u001b[0;36madvance_time_step\u001b[0;34m(model, env, brain_name, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# get the new state & reward based on this action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m \u001b[0;31m#returns ndarray, one row for each agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;31m#returns list of floats, one for each agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p2/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             )\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p2/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p2/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p2/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p2/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This cell will explore several combinations of hyperparams by training all of them\n",
    "# Use a random search for the hyperparams\n",
    "\n",
    "TIME_STEPS         = 600\n",
    "SAVE_ANALYSIS      = False\n",
    "MODEL_DISPLAY_STEP = 0 #200k is approx 10k episodes at bad_step_prob = 0.01\n",
    "\n",
    "if EXPLORE:\n",
    "    \n",
    "    # fixed for the session:\n",
    "    RUN_PREFIX        = \"M38\"\n",
    "    EPISODES          = 15001\n",
    "    NUM_RUNS          = 40\n",
    "    BAD_STEP_PROB     = 0.1\n",
    "    BUFFER_PRIME_SIZE = 5000\n",
    "    WEIGHT_DECAY      = 0.0 #was 1.0e-5\n",
    "    GAMMA             = 0.99\n",
    "    LR_ANNEAL_FREQ    = 2000 #episodes\n",
    "    LR_ANNEAL_MULT    = 0.8\n",
    "    SEED              = 44939 #(0, 111, 468, 5555, 23100, 44939)\n",
    "    \n",
    "    # session variables:\n",
    "    vars = [\n",
    "            [\"continuous-float\", 0.15,     0.23],     #BAD_STEP_PROB\n",
    "            [\"continuous-float\", 0.999924, 0.999999], #NOISE_DECAY\n",
    "            [\"continuous-float\", 0.036,    0.054],    #NOISE_SCALE\n",
    "            [\"continuous-float\", 0.000100, 0.001000], #LR_ACTOR  (was 0.000010, 0.000080)\n",
    "            [\"continuous-float\", 0.000050, 0.001000], #LR_CRITIC (was 0.000001, 0.000020)\n",
    "            [\"continuous-int\",   50,       101],      #LEARN_EVERY\n",
    "            [\"continuous-int\",   1,        3],        #LEARN_ITER\n",
    "            [\"continuous-float\", 0.00100,  0.00300],  #TAU\n",
    "            [\"discrete\",         128, 256]            #BATCH\n",
    "           ]\n",
    "    rs = RandomSampler(vars)\n",
    "    \n",
    "    print(\"Ready to train {} over {} training sets for {} episodes each, with fixed params:\"\n",
    "          .format(RUN_PREFIX, NUM_RUNS, EPISODES))\n",
    "    print(\"    Max episodes   = \", EPISODES)\n",
    "    print(\"    Weight decay   = \", WEIGHT_DECAY)\n",
    "    print(\"    Gamma          = \", GAMMA)\n",
    "    print(\"    LR anneal freq = \", LR_ANNEAL_FREQ)\n",
    "    print(\"    LR anneal mult = \", LR_ANNEAL_MULT)\n",
    "    print(\"    Buf prime size = \", BUFFER_PRIME_SIZE)\n",
    "            \n",
    "    for set_id in range(NUM_RUNS):\n",
    "        \n",
    "        # sample the variables\n",
    "        v = rs.sample()\n",
    "        #BAD_STEP_PROB = v[0]\n",
    "        NOISE_DECAY   = v[1]\n",
    "        NOISE_SCALE   = v[2]\n",
    "        LR_ACTOR      = v[3]\n",
    "        LR_CRITIC     = v[4]\n",
    "        LEARN_EVERY   = v[5]\n",
    "        LEARN_ITER    = v[6]\n",
    "        TAU           = v[7]\n",
    "        BATCH         = v[8]\n",
    "\n",
    "        # set the replay buffer size to that it fills after ~5000 bad episodes\n",
    "        # (at ~14 experiences/episode), based on the bad step retention rate\n",
    "        #buffer_size = int(60000 - 50000*(1.0 - BAD_STEP_PROB))\n",
    "        buffer_size = 100000\n",
    "\n",
    "        RUN_NAME = \"{}.{:02d}\".format(RUN_PREFIX, set_id)\n",
    "        print(\"\\n///// Beginning training set \", RUN_NAME, \" with:\")\n",
    "        print(\"      Batch size       = {:d}\".format(BATCH))\n",
    "        print(\"      Buffer size      = {:d}\".format(buffer_size))\n",
    "        print(\"      Bad step prob    = {:.4f}\".format(BAD_STEP_PROB))\n",
    "        print(\"      Noise decay      = {:.6f}\".format(NOISE_DECAY))\n",
    "        print(\"      Noise scale      = {:.3f}\".format(NOISE_SCALE))\n",
    "        print(\"      LR actor         = {:.7f}\".format(LR_ACTOR))\n",
    "        print(\"      LR critic        = {:.7f}\".format(LR_CRITIC))\n",
    "        print(\"      Learning every     \", LEARN_EVERY, \" time steps\")\n",
    "        print(\"      Learn iterations = \", LEARN_ITER)\n",
    "        print(\"      Tau              = {:.5f}\".format(TAU))\n",
    "        print(\"      Seed             = \", SEED)\n",
    "\n",
    "        ##### instantiate the agents and perform the training\n",
    "\n",
    "        maddpg = Maddpg(state_size, action_size, 2, bad_step_prob=BAD_STEP_PROB,\n",
    "                        random_seed=SEED, batch_size=BATCH, buffer_size=buffer_size,\n",
    "                        noise_decay=NOISE_DECAY, buffer_prime_size=BUFFER_PRIME_SIZE,\n",
    "                        learn_every=LEARN_EVERY, \n",
    "                        learn_iter=LEARN_ITER, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC,\n",
    "                        lr_anneal_freq=LR_ANNEAL_FREQ, lr_anneal_mult=LR_ANNEAL_MULT,\n",
    "                        weight_decay=WEIGHT_DECAY, gamma=GAMMA, noise_scale=NOISE_SCALE,\n",
    "                        tau=TAU, model_display_step=MODEL_DISPLAY_STEP)\n",
    "        \n",
    "        if config_name != None:\n",
    "            print(\"///// Beginning training from checkpoint for {}, episode {}\" \\\n",
    "                  .format(tag, initial_episode))\n",
    "            maddpg.restore_checkpoint(checkpoint_path, tag, initial_episode)\n",
    "\n",
    "        scores, avgs = train(maddpg, env, run_name=RUN_NAME, starting_episode=initial_episode,\n",
    "                             max_episodes=EPISODES, winning_score=0.5, max_time_steps=TIME_STEPS,\n",
    "                             checkpoint_interval=2000)\n",
    "\n",
    "        ##### plot the training reward history\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(scores)), scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(avgs)), avgs)\n",
    "        plt.ylabel('Avg Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "\n",
    "        ##### store the action/noise data, if being used\n",
    "\n",
    "        if SAVE_ANALYSIS:\n",
    "            maddpg.save_anal_data(RUN_PREFIX)\n",
    "\n",
    "                            \n",
    "    print(\"\\n\\nDONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEY JOHN - TODO!\n",
    "\n",
    "- update main.py to match the above code {ALL CELLS}\n",
    "- Test running from cmd line (may need a script?)\n",
    "- Clean up the bottom part of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run two trained agents against each other (inference mode)\n",
    "\n",
    "Note:  before running this cell, the Unity environment object will need to be defined (at top of notebook) with `no_graphics=False` so that the graphical game display will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not EXPLORE:\n",
    "    \n",
    "    # load the pre-trained model\n",
    "    model = Maddpg(state_size, action_size, 2)\n",
    "    model.restore_checkpoint(checkpoint_path, tag, initial_episode)\n",
    "\n",
    "    for i in range(10):                                        # play game for several episodes\n",
    "        env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        num_steps = 0\n",
    "        while True:\n",
    "            actions = model.act(states, add_noise=False)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            num_steps += 1\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        print('Episode {}: {:5.3f}, took {} steps'.format(i, np.max(scores), num_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
