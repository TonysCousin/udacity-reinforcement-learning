{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs John's solution for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.  It uses the Unity ML-Agents environment to train two cooperative agents to play a tennis-like game.\n",
    "\n",
    "**Need more description here - refer to readme?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code depends upon a custom Unity environment provided by the Udacity staff that embodies the variation on tennis.  It will open a separate Unity window for visualizing the environment as the agents train or play.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how we will use this notebook - JOHN FIX THIS!!!!\n",
    "\n",
    "In the next cell, set the appropriate values of a couple control variables:\n",
    "- **EXPLORE** determines whether the notebook does exploratory training or inference demonstration.\n",
    "    - **True** runs a hyperparameter exploration loop to generate many training runs with a random search algorithm.  To use this well, you should study that cell and specify the ranges of hyperparameters to be explored.\n",
    "    - **False** runs a few inference episodes of a pretrained model and opens a visualization window to watch it play.\n",
    "- **config_name:** the name of a model configuration & run to be loaded from a checkpoint to begin the exercise.  \n",
    "    - If EXPLORE = True, this is optional, and tells the training loop to start from this pre-trained model and continue refining it; if the value is _None_ then the training starts from a randomly initialized model.\n",
    "    - If EXPLORE = False, then this must reflect the name of a legitimate config/run (e.g. \"M37.01\").\n",
    "- **checkpoint_episode:** if a checkpoint is being used to start the exercise, then this number reflects what episode that checkpoint was captured from.  The checkpoint_name and checkpoint_episode together are required to completely identify the checkpoint file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORE            = True\n",
    "config_name        = None # Must be None if not using!\n",
    "run_number         = 0\n",
    "checkpoint_episode = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "from train import train\n",
    "from maddpg import Maddpg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "initial_episode = checkpoint_episode\n",
    "checkpoint_path = \"checkpoint/{}/\".format(config_name)\n",
    "tag = \"{}.{:02d}\".format(config_name, run_number)\n",
    "\n",
    "if EXPLORE:\n",
    "    turn_off_graphics = True\n",
    "    initial_episode = 0\n",
    "    unity_train_mode = True\n",
    "    if config_name != None:\n",
    "        initial_episode = checkpoint_episode\n",
    "else:\n",
    "    turn_off_graphics = False\n",
    "    unity_train_mode = False\n",
    "\n",
    "# create a new Unity environment\n",
    "# it needs to be done once, outside any loop, as closing an environment then restarting causes\n",
    "# a Unity exception about the handle no longer being active.\n",
    "env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\", seed=0, \n",
    "                       no_graphics=turn_off_graphics)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]                       \n",
    "env_info = env.reset(train_mode=unity_train_mode)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agents\n",
    "\n",
    "The next cells will invoke the training program to create the agents.  All of the real code is in Python flat files in this project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 \n",
      " [33, 500, 43, 5, 0.47859854191681983, -0.8729684231316783, 3.216798364512516]\n",
      "\n",
      " 1 \n",
      " [99, 500, 43, 3, 0.33557128469106057, -0.10537008359112043, -0.2473513749465578]\n",
      "\n",
      " 2 \n",
      " [101, 500, 43, 4, 0.381874512222778, -2.3007667558389944, -0.6822364877528551]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RandomSampler():\n",
    "    \n",
    "    def __init__(self, vars):\n",
    "        \"\"\"Accepts definition of the set of variables to be sampled.\n",
    "            \n",
    "            Params:\n",
    "                vars (list of lists): each item is a list containing:\n",
    "                                        item 0 - either 'discrete', 'continuous-int' or 'continuous-float'\n",
    "                                        items 1-N depend on the value of item 0:\n",
    "                                        if discrete, then these are the set of values to be chosen from\n",
    "                                        if continuous then these are the min & max bounds of the range\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vars = vars\n",
    "        \n",
    "        self.rng = np.random.default_rng()\n",
    "    \n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Draws a random sample of all variables at its disposal.\n",
    "        \n",
    "            Returns a list of values in the order of definition.\n",
    "        \"\"\"\n",
    "\n",
    "        rtn = []\n",
    "        for v in self.vars:\n",
    "            if v[0] == \"discrete\":\n",
    "                choice = self.rng.integers(low=1, high=len(v), size=1)[0]\n",
    "                rtn.append(v[choice])\n",
    "                \n",
    "            elif v[0] == \"continuous-int\":\n",
    "                choice = self.rng.integers(low=v[1], high=v[2], size=1)[0]\n",
    "                rtn.append(choice)\n",
    "                \n",
    "            elif v[0] == \"continuous-float\":\n",
    "                choice = self.rng.random() * (v[2]-v[1]) + v[1]\n",
    "                rtn.append(choice)\n",
    "            \n",
    "            else:\n",
    "                print(\"///// RandomSampler error:  unknown type \", v[0])\n",
    "            \n",
    "        return rtn\n",
    "                \n",
    "vars = [[\"discrete\", 88, 66, 11, 22, 33, 44, 99, 101, 77],\n",
    "        [\"discrete\", 500], #1-item list\n",
    "        [\"continuous-int\", 43, 44], #1-item range\n",
    "        [\"continuous-int\", 0, 10],\n",
    "        [\"continuous-float\", 0.0, 1.0],\n",
    "        [\"continuous-float\", -3.3, 0.0],\n",
    "        [\"continuous-float\", -1.0, 6.4],\n",
    "       ]\n",
    "rs = RandomSampler(vars)\n",
    "\n",
    "for i in range(3):\n",
    "    out = rs.sample()\n",
    "    print(\"\\n\", i, \"\\n\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to train M44 over 30 training sets for 12001 episodes each, with fixed params:\n",
      "    Max episodes   =  12001\n",
      "    Weight decay   =  1e-05\n",
      "    Gamma          =  0.99\n",
      "    LR anneal freq =  10000\n",
      "    LR anneal mult =  1.0\n",
      "    Buf prime size =  5000\n",
      "\n",
      "///// Beginning training set  M44.00  with:\n",
      "      Batch size       = 256\n",
      "      Buffer size      = 100000\n",
      "      Bad step prob    = 1.0000\n",
      "      Noise decay      = 0.999554\n",
      "      Noise scale      = 0.020\n",
      "      LR actor         = 0.0000802\n",
      "      LR critic        = 0.0000516\n",
      "      Learning every      2  time steps\n",
      "      Learn iterations =  1\n",
      "      Tau              = 0.00630\n",
      "      Seed             =  44939\n",
      "Priming the replay buffer........................!\n",
      "\n",
      "174\tRunning avg/max: 0.004/0.100,  mem:   7574/     4 ( 0.1%), avg 119.0 eps/min   \n",
      "* noise mult = 0.1\n",
      "380\tRunning avg/max: 0.000/0.000,  mem:  10559/     7 ( 0.1%), avg 119.8 eps/min   "
     ]
    }
   ],
   "source": [
    "# This cell will explore several combinations of hyperparams by training all of them\n",
    "# Use a random search for the hyperparams\n",
    "\n",
    "TIME_STEPS         = 600\n",
    "SAVE_ANALYSIS      = False\n",
    "MODEL_DISPLAY_STEP = 0 #200k is approx 10k episodes at bad_step_prob = 0.01\n",
    "\n",
    "if EXPLORE:\n",
    "    \n",
    "    # fixed for the session:\n",
    "    RUN_PREFIX        = \"M44\"\n",
    "    EPISODES          = 12001\n",
    "    NUM_RUNS          = 30\n",
    "    BUFFER_PRIME_SIZE = 5000\n",
    "    WEIGHT_DECAY      = 1.0e-5 #was 1.0e-5\n",
    "    GAMMA             = 0.99\n",
    "    LR_ANNEAL_FREQ    = 10000 #episodes\n",
    "    LR_ANNEAL_MULT    = 1.0\n",
    "    SEED              = 44939 #(0, 111, 468, 5555, 23100, 44939)\n",
    "    \n",
    "    # session variables:\n",
    "    vars = [\n",
    "            [\"discrete\",         0.15,      1.00],      #BAD_STEP_PROB\n",
    "            [\"continuous-float\", 0.999000,  0.999900],  #NOISE_DECAY\n",
    "            [\"discrete\",         0.020, 0.100, 1.000],  #NOISE_SCALE (was 0.040, 1.0)\n",
    "            [\"continuous-float\", 0.0000200, 0.0010000], #LR_ACTOR  (was 0.000010, 0.000080)\n",
    "            [\"continuous-float\", 0.08,      1.0],       #LR_RATIO (determines LR_CRITIC)\n",
    "            [\"discrete\",         2, 8, 20, 80],         #LEARN_EVERY\n",
    "            [\"continuous-int\",   1,         3],         #LEARN_ITER\n",
    "            [\"continuous-float\", 0.00100,   0.01000],   #TAU\n",
    "            [\"discrete\",         256]                   #BATCH\n",
    "           ]\n",
    "    rs = RandomSampler(vars)\n",
    "    \n",
    "    print(\"Ready to train {} over {} training sets for {} episodes each, with fixed params:\"\n",
    "          .format(RUN_PREFIX, NUM_RUNS, EPISODES))\n",
    "    print(\"    Max episodes   = \", EPISODES)\n",
    "    print(\"    Weight decay   = \", WEIGHT_DECAY)\n",
    "    print(\"    Gamma          = \", GAMMA)\n",
    "    print(\"    LR anneal freq = \", LR_ANNEAL_FREQ)\n",
    "    print(\"    LR anneal mult = \", LR_ANNEAL_MULT)\n",
    "    print(\"    Buf prime size = \", BUFFER_PRIME_SIZE)\n",
    "            \n",
    "    for set_id in range(NUM_RUNS):\n",
    "        \n",
    "        # sample the variables\n",
    "        v = rs.sample()\n",
    "        BAD_STEP_PROB = v[0]\n",
    "        NOISE_DECAY   = v[1]\n",
    "        NOISE_SCALE   = v[2]\n",
    "        LR_ACTOR      = v[3]\n",
    "        LR_CRITIC     = v[4] * LR_ACTOR\n",
    "        LEARN_EVERY   = v[5]\n",
    "        LEARN_ITER    = v[6]\n",
    "        TAU           = v[7]\n",
    "        BATCH         = v[8]\n",
    "\n",
    "        # set the replay buffer size to that it fills after ~5000 bad episodes\n",
    "        # (at ~14 experiences/episode), based on the bad step retention rate\n",
    "        #buffer_size = int(60000 - 50000*(1.0 - BAD_STEP_PROB))\n",
    "        buffer_size = 100000\n",
    "\n",
    "        RUN_NAME = \"{}.{:02d}\".format(RUN_PREFIX, set_id)\n",
    "        print(\"\\n///// Beginning training set \", RUN_NAME, \" with:\")\n",
    "        print(\"      Batch size       = {:d}\".format(BATCH))\n",
    "        print(\"      Buffer size      = {:d}\".format(buffer_size))\n",
    "        print(\"      Bad step prob    = {:.4f}\".format(BAD_STEP_PROB))\n",
    "        print(\"      Noise decay      = {:.6f}\".format(NOISE_DECAY))\n",
    "        print(\"      Noise scale      = {:.3f}\".format(NOISE_SCALE))\n",
    "        print(\"      LR actor         = {:.7f}\".format(LR_ACTOR))\n",
    "        print(\"      LR critic        = {:.7f}\".format(LR_CRITIC))\n",
    "        print(\"      Learning every     \", LEARN_EVERY, \" time steps\")\n",
    "        print(\"      Learn iterations = \", LEARN_ITER)\n",
    "        print(\"      Tau              = {:.5f}\".format(TAU))\n",
    "        print(\"      Seed             = \", SEED)\n",
    "\n",
    "        ##### instantiate the agents and perform the training\n",
    "\n",
    "        maddpg = Maddpg(state_size, action_size, 2, bad_step_prob=BAD_STEP_PROB,\n",
    "                        random_seed=SEED, batch_size=BATCH, buffer_size=buffer_size,\n",
    "                        noise_decay=NOISE_DECAY, buffer_prime_size=BUFFER_PRIME_SIZE,\n",
    "                        learn_every=LEARN_EVERY, \n",
    "                        learn_iter=LEARN_ITER, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC,\n",
    "                        lr_anneal_freq=LR_ANNEAL_FREQ, lr_anneal_mult=LR_ANNEAL_MULT,\n",
    "                        weight_decay=WEIGHT_DECAY, gamma=GAMMA, noise_scale=NOISE_SCALE,\n",
    "                        tau=TAU, model_display_step=MODEL_DISPLAY_STEP)\n",
    "        \n",
    "        if config_name != None:\n",
    "            print(\"///// Beginning training from checkpoint for {}, episode {}\" \\\n",
    "                  .format(tag, initial_episode))\n",
    "            maddpg.restore_checkpoint(checkpoint_path, tag, initial_episode)\n",
    "\n",
    "        scores, avgs = train(maddpg, env, run_name=RUN_NAME, starting_episode=initial_episode,\n",
    "                             max_episodes=EPISODES, winning_score=0.5, max_time_steps=TIME_STEPS,\n",
    "                             checkpoint_interval=1000)\n",
    "\n",
    "        ##### plot the training reward history\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(scores)), scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(avgs)), avgs)\n",
    "        plt.ylabel('Avg Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "\n",
    "        ##### store the action/noise data, if being used\n",
    "\n",
    "        if SAVE_ANALYSIS:\n",
    "            maddpg.save_anal_data(RUN_PREFIX)\n",
    "\n",
    "                            \n",
    "    print(\"\\n\\nDONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEY JOHN - TODO!\n",
    "\n",
    "- update main.py to match the above code {ALL CELLS}\n",
    "- Test running from cmd line (may need a script?)\n",
    "- Clean up the bottom part of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run two trained agents against each other (inference mode)\n",
    "\n",
    "Note:  before running this cell, the Unity environment object will need to be defined (at top of notebook) with `no_graphics=False` so that the graphical game display will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not EXPLORE:\n",
    "    \n",
    "    # load the pre-trained model\n",
    "    model = Maddpg(state_size, action_size, 2)\n",
    "    model.restore_checkpoint(checkpoint_path, tag, initial_episode)\n",
    "\n",
    "    for i in range(10):                                        # play game for several episodes\n",
    "        env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        num_steps = 0\n",
    "        while True:\n",
    "            actions = model.act(states, add_noise=False)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            num_steps += 1\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        print('Episode {}: {:5.3f}, took {} steps'.format(i, np.max(scores), num_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd-p2",
   "language": "python",
   "name": "p2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
