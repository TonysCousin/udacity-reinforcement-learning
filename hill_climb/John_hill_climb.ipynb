{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Continuous - Hill Climbing Practice\n",
    "\n",
    "This notebook experiments with techniques from the Udacity DRL course for hill climbing solutions to finding an optimum policy.  Code is modified from that provided in the CEM notebook provided in class.  The goal is to solve the OpenAI Gym's mountain car environment with continuous control inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvirtualdisplay\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/05/6568620fed440941b704664b9cfe5f836ad699ac7694745e7787fbdc8063/PyVirtualDisplay-2.0-py2.py3-none-any.whl\n",
      "Collecting EasyProcess (from pyvirtualdisplay)\n",
      "  Using cached https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
      "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
      "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "!python -m pip install pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the MountanCarContinuous environment & detect computing platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    \n",
    "    # Takes in the game environment so that it can size the NN layers to match states (inputs)\n",
    "    # and actions (outputs).\n",
    "    # env:  the game environment (assumes OpenAI Gym API)\n",
    "    # h_size:  the number of neurons in the hidden layer\n",
    "    \n",
    "    def __init__(self, env, h_size=16):\n",
    "        super(Agent, self).__init__()\n",
    "        self.env = env\n",
    "        \n",
    "        # state, hidden layer, action sizes\n",
    "        self.s_size = env.observation_space.shape[0]\n",
    "        self.h_size = h_size\n",
    "        self.a_size = env.action_space.shape[0]\n",
    "        \n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.s_size, self.h_size)\n",
    "        self.fc2 = nn.Linear(self.h_size, self.a_size)\n",
    "        \n",
    "        \n",
    "    # Unmarshals and stores the weights & biases in preparation for computation\n",
    "    # weights:  a 1D tensor of all the weights & biases in the model; the first part of the list\n",
    "    #           is all the fc1 weights, then all the fc1 biases, then all the fc2 weights\n",
    "    #           and finally all the fc2 biases\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        s = self.s_size\n",
    "        h = self.h_size\n",
    "        a = self.a_size\n",
    "        \n",
    "        # separate the weights & biases for each layer\n",
    "        fc1_end = (s+1)*h\n",
    "        fc1_w = weights[: s*h].reshape(s, h)\n",
    "        fc1_b = weights[s*h : fc1_end]\n",
    "        fc2_w = weights[fc1_end : fc1_end + h*a].reshape(h, a)\n",
    "        fc2_b = weights[fc1_end + h*a :]\n",
    "        \n",
    "        # set the weights for each layer\n",
    "        self.fc1.weight.data.copy_(fc1_w.view_as(self.fc1.weight.data))\n",
    "        self.fc1.bias.data.copy_(  fc1_b.view_as(self.fc1.bias.data))\n",
    "        \n",
    "        self.fc2.weight.data.copy_(fc2_w.view_as(self.fc2.weight.data))\n",
    "        self.fc2.bias.data.copy_(  fc2_b.view_as(self.fc2.bias.data))\n",
    "    \n",
    "    \n",
    "    # Returns the length of the marshalled weights list\n",
    "    def get_weights_dim(self):\n",
    "        return (self.s_size+1)*self.h_size + (self.h_size+1)*self.a_size\n",
    "    \n",
    "    \n",
    "    # Performs the forward pass computation of the NN\n",
    "    # x:  the vector of input data (environment states)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        return x.cpu().data\n",
    "    \n",
    "    \n",
    "    # Plays one episode of the game in order to evaluate the return generated by the given policy.\n",
    "    # The NN embodies the policy definition, therefore its weights encode this policy.\n",
    "    # weights:  a 1D tensor of the NN's weights & biases\n",
    "    # gamma:  the discount factor\n",
    "    # max_t:  max number of time steps allowed in an episode\n",
    "    \n",
    "    def evaluate(self, weights, gamma=1.0, max_t=5000):\n",
    "        self.set_weights(weights)\n",
    "        episode_return = 0.0\n",
    "        state = self.env.reset()\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            action = self.forward(state)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            episode_return += reward * math.pow(gamma, t)\n",
    "            if done:\n",
    "                break\n",
    "        return episode_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Each of the hill climbing methods begins with a baseline policy, then samples one or more random perturbations around that baseline.  The difference is in how those samples are used to create a new baseline for the next learning iteration.\n",
    "\n",
    "The cross-entropy method gathers many samples, then chooses the best few of these (with the highest returns on a single episode) and averages them together to form the new baseline.\n",
    "\n",
    "The evolution method uses several samples (possibly fewer than for cross-entropy), then uses a weighted average of all of them to form the new baseline, the weighting being proportional to the return from each sample.  Thus, the samaples \"higher up the hill\" have more influence in moving the baseline.\n",
    "\n",
    "In my view, cross-entropy is a generalized improvement over the evolution method; by throwing away the lower performing samples it is effectively assigning them a weight of zero.  The down-side is that by averaging the highest performing samples, it is ignoring the differences in their returns, thus possibly watering down the potential gradient ascent.  Therefore, the approach below is a hybrid of the two:  throw out any samples with reward lower than that achieved by the baseline, then use a weighted average of the remaining samples, if any.\n",
    "\n",
    "In addition to this more aggressive hill climbing, the code uses two more techniques to help avoid getting stuck at a local maximum.\n",
    "1. Using multiple random starting points within the state space\n",
    "2. Adaptive noise scaling, that changes the size of the sample perturbations based on recent level of success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# agent:         the agent model to be trained\n",
    "# max_episodes:  max number of episodes to train\n",
    "# winning_score: the game score above which the agent is considered to be adequately trained\n",
    "# gamma:         time step discount factor\n",
    "# learn_rate:    multiplier for gradually adjusting the weights to the new experience\n",
    "# max_restarts:  max number of restarts if a solution isn't found (starts from new random weights)\n",
    "# print_every:   number of episodes between status reporting\n",
    "# num_samples:   number of randomly perturbed samples to be evaluated around the baseline\n",
    "# init_sigma:    the initial standard deviation of the sample perturbations\n",
    "# Return:  list of scores of the baseline model from each episode\n",
    "\n",
    "def train(agent, max_episodes=1000, winning_score=100.0, gamma=1.0, learn_rate=0.1, max_restarts=4,\n",
    "          print_every=10, num_samples=10, init_sigma=0.5):\n",
    "    \n",
    "    SIGMA_REDUCTION = 0.99             # noise multiplier if better samples are found\n",
    "    SIGMA_INCREASE  = 1.05             # noise multiplier if no better samples are found\n",
    "    MIN_USABLE_SIGMA = 0.001           # lowest acceptable noise value (triggers end of training)\n",
    "    MAX_USABLE_SIGMA = 1.2             # largest acceptable noise value\n",
    "    MAX_EPISODES_BAD_NOISE = 9         # num epidodes we will continue to try to find a better noise sample\n",
    "    MAX_EPISODES_BAD_SCORE = 12        # num episodes we will continue without seeing an improved score\n",
    "    \n",
    "    # loop on number of restarts\n",
    "    start = 0\n",
    "    solution_found = False\n",
    "    while not solution_found  and  start < max_restarts:\n",
    "        print(\"\\n///// Starting new NN with random weights - start {}\".format(start))\n",
    "    \n",
    "        # initialize bookkeeping\n",
    "        scores_deque = deque(maxlen=40)\n",
    "        scores = []\n",
    "        adaptive_noise = True\n",
    "        episodes_bad_noise = 0\n",
    "        episodes_bad_score = 0\n",
    "        best_score = -np.inf\n",
    "    \n",
    "        # get initial random weights & biases for the model and loop on episodes\n",
    "        sigma = init_sigma\n",
    "        min_sigma = init_sigma\n",
    "        weight_size = agent.get_weights_dim()\n",
    "        baseline = torch.from_numpy(sigma*np.random.randn(weight_size)/4.0) #float\n",
    "        for ep in range(max_episodes):\n",
    "\n",
    "            # evaluate the baseline and store its return\n",
    "            reward = agent.evaluate(baseline, gamma=gamma)\n",
    "            scores_deque.append(reward)\n",
    "            scores.append(reward)\n",
    "            if reward > best_score:\n",
    "                best_score = reward\n",
    "                episodes_bad_score = 0\n",
    "            else:\n",
    "                episodes_bad_score += 1\n",
    "\n",
    "            # print status\n",
    "            if ep % print_every == 0:\n",
    "                bnp = baseline.numpy()\n",
    "                w_avg = np.average(bnp)\n",
    "                w_std = np.std(bnp)\n",
    "                w_pos = 0\n",
    "                for item in bnp:\n",
    "                    if item > 0:\n",
    "                        w_pos += 1\n",
    "                pct_pos = 100.0 * w_pos / weight_size\n",
    "                print(\"{:3} Avg score = {:6.2f}, episode = {:6.2f}, sigma = {:.3f}, \" \\\n",
    "                      \"w.avg = {:.3f}, w.std = {:.3f}, pos = {:2.0f}%\"\n",
    "                      .format(ep, np.mean(scores_deque), reward, sigma, w_avg, w_std, pct_pos), end='\\n')\n",
    "\n",
    "            # if average scores are high enough, end the training\n",
    "            if np.mean(scores_deque) >= winning_score:\n",
    "                print(\"\\nEnvironment solved in {:d} episodes!\\tAvg Score: {:.2f}\".format(ep, \n",
    "                                                                                       np.mean(scores_deque)))\n",
    "                solution_found = True\n",
    "                break\n",
    "\n",
    "            # else if we haven't seen an improved score for some time then end the training\n",
    "            elif episodes_bad_score > MAX_EPISODES_BAD_SCORE:\n",
    "                print(\"{:3}-{} episodes without an improved score. Best episode score = {:.2f}. Terminating.\"\n",
    "                      .format(ep, episodes_bad_score, best_score))\n",
    "                break\n",
    "\n",
    "            # generate random samples around the baseline\n",
    "            # (not clear why I need to explicitly convert both terms to float; they should be already)\n",
    "            samples = torch.FloatTensor(num_samples, weight_size)\n",
    "            for i in range(num_samples):\n",
    "                samples[i] = baseline.float() + torch.from_numpy(sigma*np.random.randn(weight_size)).float()\n",
    "\n",
    "            # evaluate each sample and store its return if it is better than the baseline return\n",
    "            sample_rewards = []\n",
    "            sample_weights = []\n",
    "            for s in range(num_samples):\n",
    "                r = agent.evaluate(samples[s], gamma=gamma)\n",
    "                if r > reward:\n",
    "                    sample_rewards.append(r)\n",
    "                    sample_weights.append(samples[s])\n",
    "\n",
    "            # if at least one sample performed better than the baseline then\n",
    "            num_elite_samples = len(sample_rewards)\n",
    "            if num_elite_samples > 0:\n",
    "                #print(\"{:3} better samples\".format(num_elite_samples))\n",
    "\n",
    "                # reset the not-found counter\n",
    "                episodes_without_improvement = 0\n",
    "\n",
    "                # get the weighted average of all better samples and consider this the new baseline\n",
    "                r = torch.FloatTensor(sample_rewards).resize_(num_elite_samples, 1)\n",
    "                w = torch.zeros(num_elite_samples, weight_size).float()\n",
    "                for s in range(num_elite_samples):\n",
    "                    w[s][:] = sample_weights[s]\n",
    "                baseline = (1.0-learn_rate)*baseline.float() + learn_rate*((r*w).sum(dim=0)/r.sum(dim=0)).float()\n",
    "\n",
    "                # reduce the noise magnitude and store it as the new minimum noise\n",
    "                sigma *= SIGMA_REDUCTION\n",
    "                if sigma < min_sigma:\n",
    "                    min_sigma = sigma\n",
    "\n",
    "            # else (no samples performed as well as the baseline; we may have found the top of the hill)\n",
    "            else:\n",
    "                #print(\"*** No better samples found.\")\n",
    "\n",
    "                # increment the counter of no improvement\n",
    "                episodes_bad_noise += 1\n",
    "\n",
    "                # if we are still in adaptive noise phase then\n",
    "                if adaptive_noise:\n",
    "\n",
    "                    # if we haven't seen improvement for several episodes then\n",
    "                    if episodes_bad_noise > MAX_EPISODES_BAD_NOISE:\n",
    "\n",
    "                        # set noise to min used thus far and indicate transition to fine tuning phase\n",
    "                        # since it appears we are close to the global maximum\n",
    "                        sigma = min_sigma\n",
    "                        adaptive_noise = False\n",
    "                        print(\"{:3}-Turning off adaptive noise. Resetting sigma = {:.3f}\".format(ep, sigma))\n",
    "\n",
    "                        # reset the counter so it can be used for the fine tuning phase\n",
    "                        episodes_without_improvement = 0\n",
    "\n",
    "                    # else (still adapting the noise level)\n",
    "                    else:\n",
    "\n",
    "                        # increase the noise magnitude\n",
    "                        sigma *= SIGMA_INCREASE\n",
    "                        if sigma > MAX_USABLE_SIGMA:\n",
    "                            sigma = MAX_USABLE_SIGMA\n",
    "\n",
    "                # else (in fine tuning phase, near a peak)\n",
    "                else:\n",
    "\n",
    "                    # reduce the noise\n",
    "                    sigma *= SIGMA_REDUCTION\n",
    "\n",
    "                    # if we've hit the smallest noise we care to deal with then terminate\n",
    "                    if sigma < MIN_USABLE_SIGMA:\n",
    "                        print(\"{:3}-Fine tuning is at minimum noise. Best score = {:.2f}. \" \\\n",
    "                              \"Terminating search.\".format(ep, best_score))\n",
    "                        break\n",
    "\n",
    "                    # increment the final phase counter and terminate after enough with no improvement\n",
    "                    if episodes_bad_noise > MAX_EPISODES_BAD_NOISE:\n",
    "                        print(\"{:3}-{} episodes of fine tuning without better noise samples. \" \\\n",
    "                              \"Best score = {:.2f}. Terminating search.\".\n",
    "                             format(ep, best_score, episodes_bad_noise))\n",
    "                        break\n",
    "                        \n",
    "        start += 1\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "///// Starting new NN with random weights - start 0\n",
      "  0 Avg score =  -0.01, episode =  -0.01, sigma = 0.500, w.avg = 0.006, w.std = 0.123, pos = 60%\n",
      "  5 Avg score =  -0.00, episode =  -0.00, sigma = 0.602, w.avg = -0.000, w.std = 0.122, pos = 54%\n",
      " 10 Avg score =  -0.01, episode =  -0.05, sigma = 0.724, w.avg = 0.008, w.std = 0.159, pos = 49%\n",
      "Turning off adaptive noise. Resetting sigma = 0.495\n",
      "11 episodes of fine tuning without better noise samples. Terminating search.\n",
      "\n",
      "///// Starting new NN with random weights - start 1\n",
      "  0 Avg score =  -0.00, episode =  -0.00, sigma = 0.500, w.avg = 0.007, w.std = 0.121, pos = 45%\n",
      "  5 Avg score =  -0.00, episode =  -0.00, sigma = 0.638, w.avg = 0.007, w.std = 0.121, pos = 45%\n",
      "Turning off adaptive noise. Resetting sigma = 0.500\n",
      " 10 Avg score =  -0.00, episode =  -0.00, sigma = 0.500, w.avg = 0.007, w.std = 0.121, pos = 45%\n",
      "11 episodes of fine tuning without better noise samples. Terminating search.\n",
      "\n",
      "///// Starting new NN with random weights - start 2\n",
      "  0 Avg score =  -0.73, episode =  -0.73, sigma = 0.500, w.avg = 0.019, w.std = 0.129, pos = 57%\n",
      "  5 Avg score =  -0.91, episode =  -1.84, sigma = 0.475, w.avg = 0.024, w.std = 0.149, pos = 60%\n",
      " 10 Avg score =  -1.58, episode =  -2.00, sigma = 0.452, w.avg = 0.018, w.std = 0.165, pos = 55%\n",
      "13 episodes without an improved score. Best episode score = -0.58. Terminating.\n",
      "\n",
      "///// Starting new NN with random weights - start 3\n",
      "  0 Avg score =  -0.15, episode =  -0.15, sigma = 0.500, w.avg = -0.006, w.std = 0.102, pos = 45%\n",
      "  5 Avg score =  -0.37, episode =  -0.88, sigma = 0.535, w.avg = -0.002, w.std = 0.133, pos = 42%\n",
      " 10 Avg score =  -0.58, episode =  -0.00, sigma = 0.509, w.avg = -0.020, w.std = 0.149, pos = 40%\n",
      " 15 Avg score =  -0.40, episode =  -0.00, sigma = 0.649, w.avg = -0.020, w.std = 0.149, pos = 40%\n",
      "Turning off adaptive noise. Resetting sigma = 0.495\n",
      "11 episodes of fine tuning without better noise samples. Terminating search.\n"
     ]
    }
   ],
   "source": [
    "seed = 4\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "agent = Agent(env).to(device)\n",
    "\n",
    "scores = train(agent, learn_rate=0.1, init_sigma=0.5, num_samples=50, print_every=5, winning_score=90.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
