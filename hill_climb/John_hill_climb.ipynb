{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Continuous - Hill Climbing Practice\n",
    "\n",
    "This notebook experiments with techniques from the Udacity DRL course for hill climbing solutions to finding an optimum policy.  Code is modified from that provided in the CEM notebook provided in class.  The goal is to solve the OpenAI Gym's mountain car environment with continuous control inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvirtualdisplay\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/05/6568620fed440941b704664b9cfe5f836ad699ac7694745e7787fbdc8063/PyVirtualDisplay-2.0-py2.py3-none-any.whl\n",
      "Collecting EasyProcess (from pyvirtualdisplay)\n",
      "  Using cached https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
      "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
      "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "!python -m pip install pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the MountanCarContinuous environment & detect computing platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    \n",
    "    # Takes in the game environment so that it can size the NN layers to match states (inputs)\n",
    "    # and actions (outputs).\n",
    "    # env:  the game environment (assumes OpenAI Gym API)\n",
    "    # h_size:  the number of neurons in the hidden layer\n",
    "    \n",
    "    def __init__(self, env, h_size=16):\n",
    "        super(Agent, self).__init__()\n",
    "        self.env = env\n",
    "        \n",
    "        # state, hidden layer, action sizes\n",
    "        self.s_size = env.observation_space.shape[0]\n",
    "        self.h_size = h_size\n",
    "        self.a_size = env.action_space.shape[0]\n",
    "        \n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.s_size, self.h_size)\n",
    "        self.fc2 = nn.Linear(self.h_size, self.a_size)\n",
    "        \n",
    "        \n",
    "    # Unmarshals and stores the weights & biases in preparation for computation\n",
    "    # weights:  a 1D tensor of all the weights & biases in the model; the first part of the list\n",
    "    #           is all the fc1 weights, then all the fc1 biases, then all the fc2 weights\n",
    "    #           and finally all the fc2 biases\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        s = self.s_size\n",
    "        h = self.h_size\n",
    "        a = self.a_size\n",
    "        \n",
    "        # separate the weights & biases for each layer\n",
    "        fc1_end = (s+1)*h\n",
    "        fc1_w = weights[: s*h].reshape(s, h)\n",
    "        fc1_b = weights[s*h : fc1_end]\n",
    "        fc2_w = weights[fc1_end : fc1_end + h*a].reshape(h, a)\n",
    "        fc2_b = weights[fc1_end + h*a :]\n",
    "        \n",
    "        # set the weights for each layer\n",
    "        self.fc1.weight.data.copy_(fc1_w.view_as(self.fc1.weight.data))\n",
    "        self.fc1.bias.data.copy_(  fc1_b.view_as(self.fc1.bias.data))\n",
    "        \n",
    "        self.fc2.weight.data.copy_(fc2_w.view_as(self.fc2.weight.data))\n",
    "        self.fc2.bias.data.copy_(  fc2_b.view_as(self.fc2.bias.data))\n",
    "    \n",
    "    \n",
    "    # Returns the length of the marshalled weights list\n",
    "    def get_weights_dim(self):\n",
    "        return (self.s_size+1)*self.h_size + (self.h_size+1)*self.a_size\n",
    "    \n",
    "    \n",
    "    # Performs the forward pass computation of the NN\n",
    "    # x:  the vector of input data (environment states)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        return x.cpu().data\n",
    "    \n",
    "    \n",
    "    # Plays one episode of the game in order to evaluate the return generated by the given policy.\n",
    "    # The NN embodies the policy definition, therefore its weights encode this policy.\n",
    "    # weights:  a 1D tensor of the NN's weights & biases\n",
    "    # gamma:  the discount factor\n",
    "    # max_t:  max number of time steps allowed in an episode\n",
    "    \n",
    "    def evaluate(self, weights, gamma=1.0, max_t=5000):\n",
    "        self.set_weights(weights)\n",
    "        episode_return = 0.0\n",
    "        state = self.env.reset()\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            action = self.forward(state)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            episode_return += reward * math.pow(gamma, t)\n",
    "            if done:\n",
    "                break\n",
    "        return episode_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Each of the hill climbing methods begins with a baseline policy, then samples one or more random perturbations around that baseline.  The difference is in how those samples are used to create a new baseline for the next learning iteration.\n",
    "\n",
    "The cross-entropy method gathers many samples, then chooses the best few of these (with the highest returns on a single episode) and averages them together to form the new baseline.\n",
    "\n",
    "The evolution method uses several samples (possibly fewer than for cross-entropy), then uses a weighted average of all of them to form the new baseline, the weighting being proportional to the return from each sample.  Thus, the samaples \"higher up the hill\" have more influence in moving the baseline.\n",
    "\n",
    "In my view, cross-entropy is a generalized improvement over the evolution method; by throwing away the lower performing samples it is effectively assigning them a weight of zero.  The down-side is that by averaging the highest performing samples, it is ignoring the differences in their returns, thus possibly watering down the potential gradient ascent.  Therefore, the approach below is a hybrid of the two:  throw out any samples with reward lower than that achieved by the baseline, then use a weighted average of the remaining samples, if any.\n",
    "\n",
    "In addition to this more aggressive hill climbing, the code uses two more techniques to help avoid getting stuck at a local maximum.\n",
    "1. Using multiple random starting points within the state space\n",
    "2. Adaptive noise scaling, that changes the size of the sample perturbations based on recent level of success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# agent:       the agent model to be trained\n",
    "# max_epochs:  max number of epochs to train\n",
    "# winning_score: the game score above which the agent is considered to be adequately trained\n",
    "# gamma:       time step discount factor\n",
    "# print_every: number of epochs between status reporting\n",
    "# num_samples: number of randomly perturbed samples to be evaluated around the baseline\n",
    "# init_sigma:  the initial standard deviation of the sample perturbations\n",
    "# Return:  list of scores of the baseline model from each epoch\n",
    "\n",
    "def train(agent, max_epochs=1000, winning_score=100.0, gamma=1.0, print_every=10, \n",
    "          num_samples=10, init_sigma=0.5):\n",
    "    \n",
    "    SIGMA_REDUCTION = 0.995            # noise multiplier if better samples are found\n",
    "    SIGMA_INCREASE  = 1.5              # noise multiplier if no better samples are found\n",
    "    MIN_USABLE_SIGMA = 0.001           # lowest acceptable noise value (triggers end of training)\n",
    "    MAX_EPOCHS_WITHOUT_IMPROVEMENT = 9 # num epochs we will continue to try to find an improvement\n",
    "    \n",
    "    adaptive_noise = True\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # store the most recent 100 epoch returns\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    \n",
    "    # get initial random weights & biases for the model and loop on epochs\n",
    "    sigma = init_sigma\n",
    "    min_sigma = init_sigma\n",
    "    weight_size = agent.get_weights_dim()\n",
    "    baseline = torch.from_numpy(sigma*np.random.randn(weight_size)) #float\n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        # evaluate the baseline and store its return\n",
    "        reward = agent.evaluate(baseline, gamma=gamma)\n",
    "        scores_deque.append(reward)\n",
    "        scores.append(reward)\n",
    "        \n",
    "        # print status\n",
    "        if epoch % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score = {:.2f}, sigma = {:.3f}.   '\n",
    "                  .format(epoch, np.mean(scores_deque), sigma), end='')\n",
    "\n",
    "        # if average scores are high enough, end the training\n",
    "        if np.mean(scores_deque) >= winning_score:\n",
    "            print('\\nEnvironment solved in {:d} epochs!\\tAvg Score: {:.2f}'.format(epoch, \n",
    "                                                                                   np.mean(scores_deque)))\n",
    "            break\n",
    "\n",
    "        # generate random samples around the baseline\n",
    "        # (not clear why I need to explicitly convert both terms to float; they should be already)\n",
    "        samples = torch.FloatTensor(num_samples, weight_size)\n",
    "        for i in range(num_samples):\n",
    "            samples[i] = baseline.float() + torch.from_numpy(sigma*np.random.randn(weight_size)).float()\n",
    "        \n",
    "        # evaluate each sample and store its return if it is better than the baseline return\n",
    "        sample_rewards = []\n",
    "        sample_weights = []\n",
    "        for s in range(num_samples):\n",
    "            r = agent.evaluate(samples[s], gamma=gamma)\n",
    "            if r > reward:\n",
    "                sample_rewards.append(r)\n",
    "                sample_weights.append(samples[s])\n",
    "        \n",
    "        # if at least one sample performed better than the baseline then\n",
    "        num_elite_samples = len(sample_rewards)\n",
    "        if num_elite_samples > 0:\n",
    "            print(\"{:3} better samples\".format(num_elite_samples))\n",
    "            \n",
    "            # reset the not-found counter\n",
    "            epochs_without_improvement = 0\n",
    "        \n",
    "            # get the weighted average of all better samples and consider this the new baseline\n",
    "            r = torch.FloatTensor(sample_rewards).resize_(num_elite_samples, 1)\n",
    "            w = torch.zeros(num_elite_samples, weight_size).float()\n",
    "            for s in range(num_elite_samples):\n",
    "                w[s][:] = sample_weights[s]\n",
    "            baseline = (r*w).sum(dim=0) / r.sum(dim=0)\n",
    "            \n",
    "            # reduce the noise magnitude and store it as the new minimum noise\n",
    "            sigma *= SIGMA_REDUCTION\n",
    "            if sigma < min_sigma:\n",
    "                min_sigma = sigma\n",
    "            \n",
    "        # else (no samples performed as well as the baseline; we may have found the top of the hill)\n",
    "        else:\n",
    "            print(\"*** No better samples found.\")\n",
    "        \n",
    "            # increment the counter of no improvement\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "            # if we are still in adaptive noise phase then\n",
    "            if adaptive_noise:\n",
    "        \n",
    "                # if we haven't seen improvement for several epochs then\n",
    "                if epochs_without_improvement > MAX_EPOCHS_WITHOUT_IMPROVEMENT:\n",
    "                \n",
    "                    # set noise to min used thus far and indicate transition to fine tuning phase\n",
    "                    # since it appears we are close to the global maximum\n",
    "                    sigma = min_sigma\n",
    "                    adaptive_noise = False\n",
    "                    print(\"Turning off adaptive noise: sigma = \", sigma)\n",
    "                    \n",
    "                    # reset the counter so it can be used for the fine tuning phase\n",
    "                    epochs_without_improvement = 0\n",
    "                    \n",
    "                # else (still adapting the noise level)\n",
    "                else:\n",
    "\n",
    "                    # increase the noise magnitude\n",
    "                    sigma *= SIGMA_INCREASE\n",
    "                \n",
    "            # else (in fine tuning phase, near a peak)\n",
    "            else:\n",
    "            \n",
    "                # reduce the noise\n",
    "                sigma *= SIGMA_REDUCTION\n",
    "                \n",
    "                # if we've hit the smallest noise we care to deal with then terminate\n",
    "                if sigma < MIN_USABLE_SIGMA:\n",
    "                    print(\"Fine tuning is a minimum noise. Terminating search.\")\n",
    "                    break\n",
    "                \n",
    "                # increment the final phase counter and terminate after enough with no improvement\n",
    "                if epochs_without_improvement > MAX_EPOCHS_WITHOUT_IMPROVEMENT:\n",
    "                    print(\"{} epochs of fine tuning without improvement. Terminating search.\".\n",
    "                         format(epochs_without_improvement))\n",
    "                    break\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score = -98.53, sigma = 1.000.    16 better samples\n",
      "Episode 1\tAverage Score = -89.56, sigma = 0.990.     5 better samples\n",
      "Episode 2\tAverage Score = -93.01, sigma = 0.980.    12 better samples\n",
      "Episode 3\tAverage Score = -93.89, sigma = 0.970.     2 better samples\n",
      "Episode 4\tAverage Score = -95.09, sigma = 0.961.    20 better samples\n",
      "Episode 5\tAverage Score = -95.89, sigma = 0.951.    10 better samples\n",
      "Episode 6\tAverage Score = -96.46, sigma = 0.941.     7 better samples\n",
      "Episode 7\tAverage Score = -96.88, sigma = 0.932.     7 better samples\n",
      "Episode 8\tAverage Score = -97.13, sigma = 0.923.     5 better samples\n",
      "Episode 9\tAverage Score = -97.10, sigma = 0.914.     6 better samples\n",
      "Episode 10\tAverage Score = -97.35, sigma = 0.904.    15 better samples\n",
      "Episode 11\tAverage Score = -97.57, sigma = 0.895.    13 better samples\n",
      "Episode 12\tAverage Score = -87.67, sigma = 0.886.     2 better samples\n",
      "Episode 13\tAverage Score = -80.20, sigma = 0.878.     2 better samples\n",
      "Episode 14\tAverage Score = -69.79, sigma = 0.869.   *** No better samples found.\n",
      "Episode 15\tAverage Score = -60.57, sigma = 1.737.   *** No better samples found.\n",
      "Episode 16\tAverage Score = -51.78, sigma = 3.475.   *** No better samples found.\n",
      "Episode 17\tAverage Score = -44.02, sigma = 6.950.   *** No better samples found.\n",
      "Episode 18\tAverage Score = -37.68, sigma = 13.900.   *** No better samples found.\n",
      "Episode 19\tAverage Score = -31.36, sigma = 27.800.   *** No better samples found.\n",
      "Turning off adaptive noise: sigma =  0.8687458127689781\n",
      "Episode 20\tAverage Score = -26.16, sigma = 0.869.     1 better samples\n",
      "Episode 21\tAverage Score = -29.47, sigma = 0.860.     7 better samples\n",
      "Episode 22\tAverage Score = -32.54, sigma = 0.851.     7 better samples\n",
      "Episode 23\tAverage Score = -35.34, sigma = 0.843.    11 better samples\n",
      "Episode 24\tAverage Score = -37.92, sigma = 0.835.    12 better samples\n",
      "Episode 25\tAverage Score = -40.31, sigma = 0.826.    12 better samples\n",
      "Episode 26\tAverage Score = -42.52, sigma = 0.818.    21 better samples\n",
      "Episode 27\tAverage Score = -44.56, sigma = 0.810.    15 better samples\n",
      "Episode 28\tAverage Score = -46.39, sigma = 0.802.     1 better samples\n",
      "Episode 29\tAverage Score = -48.17, sigma = 0.794.     5 better samples\n",
      "Episode 30\tAverage Score = -49.84, sigma = 0.786.     9 better samples\n",
      "Episode 31\tAverage Score = -51.26, sigma = 0.778.     2 better samples\n",
      "Episode 32\tAverage Score = -52.73, sigma = 0.770.    15 better samples\n",
      "Episode 33\tAverage Score = -54.11, sigma = 0.762.     5 better samples\n",
      "Episode 34\tAverage Score = -55.15, sigma = 0.755.     5 better samples\n",
      "Episode 35\tAverage Score = -56.20, sigma = 0.747.    20 better samples\n",
      "Episode 36\tAverage Score = -56.94, sigma = 0.740.     8 better samples\n",
      "Episode 37\tAverage Score = -56.37, sigma = 0.732.     8 better samples\n",
      "Episode 38\tAverage Score = -55.11, sigma = 0.725.     2 better samples\n",
      "Episode 39\tAverage Score = -54.04, sigma = 0.718.     6 better samples\n",
      "Episode 40\tAverage Score = -52.80, sigma = 0.711.     6 better samples\n",
      "Episode 41\tAverage Score = -51.67, sigma = 0.703.     3 better samples\n",
      "Episode 42\tAverage Score = -50.60, sigma = 0.696.     4 better samples\n",
      "Episode 43\tAverage Score = -49.46, sigma = 0.689.     1 better samples\n",
      "Episode 44\tAverage Score = -48.47, sigma = 0.683.     4 better samples\n",
      "Episode 45\tAverage Score = -47.45, sigma = 0.676.     1 better samples\n",
      "Episode 46\tAverage Score = -46.50, sigma = 0.669.   *** No better samples found.\n",
      "Episode 47\tAverage Score = -45.61, sigma = 0.662.     3 better samples\n",
      "Episode 48\tAverage Score = -44.83, sigma = 0.656.     6 better samples\n",
      "Episode 49\tAverage Score = -44.05, sigma = 0.649.     8 better samples\n",
      "Episode 50\tAverage Score = -43.28, sigma = 0.643.     2 better samples\n",
      "Episode 51\tAverage Score = -42.46, sigma = 0.636.   *** No better samples found.\n",
      "Episode 52\tAverage Score = -41.75, sigma = 0.630.     2 better samples\n",
      "Episode 53\tAverage Score = -41.04, sigma = 0.624.     7 better samples\n",
      "Episode 54\tAverage Score = -40.37, sigma = 0.617.     5 better samples\n",
      "Episode 55\tAverage Score = -39.66, sigma = 0.611.   *** No better samples found.\n",
      "Episode 56\tAverage Score = -39.06, sigma = 0.605.     8 better samples\n",
      "Episode 57\tAverage Score = -38.41, sigma = 0.599.     1 better samples\n",
      "Episode 58\tAverage Score = -37.80, sigma = 0.593.     2 better samples\n",
      "Episode 59\tAverage Score = -37.21, sigma = 0.587.     3 better samples\n",
      "Episode 60\tAverage Score = -36.64, sigma = 0.581.   *** No better samples found.\n",
      "Episode 61\tAverage Score = -36.05, sigma = 0.575.   *** No better samples found.\n",
      "Episode 62\tAverage Score = -35.51, sigma = 0.570.     4 better samples\n",
      "Episode 63\tAverage Score = -34.98, sigma = 0.564.     1 better samples\n",
      "Episode 64\tAverage Score = -34.49, sigma = 0.558.     3 better samples\n",
      "Episode 65\tAverage Score = -34.16, sigma = 0.553.    13 better samples\n",
      "Episode 66\tAverage Score = -33.69, sigma = 0.547.     1 better samples\n",
      "Episode 67\tAverage Score = -33.37, sigma = 0.542.     7 better samples\n",
      "Episode 68\tAverage Score = -33.00, sigma = 0.536.     6 better samples\n",
      "Episode 69\tAverage Score = -32.59, sigma = 0.531.     3 better samples\n",
      "Episode 70\tAverage Score = -32.37, sigma = 0.526.    17 better samples\n",
      "Episode 71\tAverage Score = -31.95, sigma = 0.520.     3 better samples\n",
      "Episode 72\tAverage Score = -31.63, sigma = 0.515.     6 better samples\n",
      "Episode 73\tAverage Score = -31.22, sigma = 0.510.     2 better samples\n",
      "Episode 74\tAverage Score = -30.96, sigma = 0.505.    18 better samples\n",
      "Episode 75\tAverage Score = -30.67, sigma = 0.500.    13 better samples\n",
      "Episode 76\tAverage Score = -30.31, sigma = 0.495.     3 better samples\n",
      "Episode 77\tAverage Score = -30.06, sigma = 0.490.    15 better samples\n",
      "Episode 78\tAverage Score = -29.74, sigma = 0.485.     6 better samples\n",
      "Episode 79\tAverage Score = -29.38, sigma = 0.480.     1 better samples\n",
      "Episode 80\tAverage Score = -29.12, sigma = 0.475.    12 better samples\n",
      "Episode 81\tAverage Score = -28.86, sigma = 0.471.    13 better samples\n",
      "Episode 82\tAverage Score = -28.57, sigma = 0.466.     5 better samples\n",
      "Episode 83\tAverage Score = -28.26, sigma = 0.461.     2 better samples\n",
      "Episode 84\tAverage Score = -28.06, sigma = 0.457.    21 better samples\n",
      "Episode 85\tAverage Score = -27.88, sigma = 0.452.    19 better samples\n",
      "Episode 86\tAverage Score = -27.56, sigma = 0.448.   *** No better samples found.\n",
      "Episode 87\tAverage Score = -27.26, sigma = 0.443.   *** No better samples found.\n",
      "Episode 88\tAverage Score = -26.99, sigma = 0.439.     7 better samples\n",
      "Episode 89\tAverage Score = -26.71, sigma = 0.434.     2 better samples\n",
      "Episode 90\tAverage Score = -26.47, sigma = 0.430.     6 better samples\n",
      "Episode 91\tAverage Score = -26.19, sigma = 0.426.   *** No better samples found.\n",
      "Episode 92\tAverage Score = -25.98, sigma = 0.421.    10 better samples\n",
      "Episode 93\tAverage Score = -25.73, sigma = 0.417.     1 better samples\n",
      "Episode 94\tAverage Score = -25.55, sigma = 0.413.    20 better samples\n",
      "Episode 95\tAverage Score = -25.37, sigma = 0.409.    18 better samples\n",
      "Episode 96\tAverage Score = -25.17, sigma = 0.405.     9 better samples\n",
      "Episode 97\tAverage Score = -24.94, sigma = 0.401.     3 better samples\n",
      "Episode 98\tAverage Score = -24.75, sigma = 0.397.    11 better samples\n",
      "Episode 99\tAverage Score = -24.58, sigma = 0.393.    15 better samples\n",
      "Episode 100\tAverage Score = -23.64, sigma = 0.389.    10 better samples\n",
      "Episode 101\tAverage Score = -22.95, sigma = 0.385.    29 better samples\n",
      "Episode 102\tAverage Score = -22.01, sigma = 0.381.    10 better samples\n",
      "Episode 103\tAverage Score = -21.07, sigma = 0.377.     3 better samples\n",
      "Episode 104\tAverage Score = -20.13, sigma = 0.373.    17 better samples\n",
      "Episode 105\tAverage Score = -19.18, sigma = 0.370.    10 better samples\n",
      "Episode 106\tAverage Score = -18.19, sigma = 0.366.   *** No better samples found.\n",
      "Episode 107\tAverage Score = -17.22, sigma = 0.362.     7 better samples\n",
      "Episode 108\tAverage Score = -16.28, sigma = 0.359.    15 better samples\n",
      "Episode 109\tAverage Score = -15.34, sigma = 0.355.     8 better samples\n",
      "Episode 110\tAverage Score = -14.42, sigma = 0.352.    20 better samples\n",
      "Episode 111\tAverage Score = -13.45, sigma = 0.348.     8 better samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 112\tAverage Score = -13.86, sigma = 0.345.    29 better samples\n",
      "Episode 113\tAverage Score = -14.04, sigma = 0.341.   *** No better samples found.\n",
      "Episode 114\tAverage Score = -14.86, sigma = 0.338.    15 better samples\n",
      "Episode 115\tAverage Score = -15.64, sigma = 0.334.   *** No better samples found.\n",
      "Episode 116\tAverage Score = -16.54, sigma = 0.331.     1 better samples\n",
      "Episode 117\tAverage Score = -17.45, sigma = 0.328.     9 better samples\n",
      "Episode 118\tAverage Score = -18.29, sigma = 0.324.    21 better samples\n",
      "Episode 119\tAverage Score = -19.18, sigma = 0.321.     1 better samples\n",
      "Episode 120\tAverage Score = -20.05, sigma = 0.318.    22 better samples\n",
      "Episode 121\tAverage Score = -19.10, sigma = 0.315.    10 better samples\n",
      "Episode 122\tAverage Score = -18.13, sigma = 0.312.     7 better samples\n",
      "Episode 123\tAverage Score = -17.14, sigma = 0.309.     1 better samples\n",
      "Episode 124\tAverage Score = -16.16, sigma = 0.305.     5 better samples\n",
      "Episode 125\tAverage Score = -15.17, sigma = 0.302.     2 better samples\n",
      "Episode 126\tAverage Score = -14.20, sigma = 0.299.     6 better samples\n",
      "Episode 127\tAverage Score = -13.24, sigma = 0.296.    11 better samples\n",
      "Episode 128\tAverage Score = -12.30, sigma = 0.293.     8 better samples\n",
      "Episode 129\tAverage Score = -11.33, sigma = 0.290.     5 better samples\n",
      "Episode 130\tAverage Score = -10.40, sigma = 0.288.    24 better samples\n",
      "Episode 131\tAverage Score = -9.52, sigma = 0.285.    17 better samples\n",
      "Episode 132\tAverage Score = -8.56, sigma = 0.282.    10 better samples\n",
      "Episode 133\tAverage Score = -7.59, sigma = 0.279.    11 better samples\n",
      "Episode 134\tAverage Score = -6.72, sigma = 0.276.    12 better samples\n",
      "Episode 135\tAverage Score = -5.79, sigma = 0.273.   *** No better samples found.\n",
      "Episode 136\tAverage Score = -4.97, sigma = 0.271.     3 better samples\n",
      "Episode 137\tAverage Score = -4.66, sigma = 0.268.    12 better samples\n",
      "Episode 138\tAverage Score = -4.62, sigma = 0.265.    14 better samples\n",
      "Episode 139\tAverage Score = -4.51, sigma = 0.263.     4 better samples\n",
      "Episode 140\tAverage Score = -4.51, sigma = 0.260.    13 better samples\n",
      "Episode 141\tAverage Score = -4.46, sigma = 0.257.   *** No better samples found.\n",
      "Episode 142\tAverage Score = -4.40, sigma = 0.255.   *** No better samples found.\n",
      "Episode 143\tAverage Score = -4.43, sigma = 0.252.    14 better samples\n",
      "Episode 144\tAverage Score = -4.41, sigma = 0.250.     7 better samples\n",
      "Episode 145\tAverage Score = -4.41, sigma = 0.247.     4 better samples\n",
      "Episode 146\tAverage Score = -4.44, sigma = 0.245.    21 better samples\n",
      "Episode 147\tAverage Score = -4.45, sigma = 0.242.    12 better samples\n",
      "Episode 148\tAverage Score = -4.48, sigma = 0.240.    35 better samples\n",
      "Episode 149\tAverage Score = -4.46, sigma = 0.238.    20 better samples\n",
      "Episode 150\tAverage Score = -4.46, sigma = 0.235.    12 better samples\n",
      "Episode 151\tAverage Score = -4.49, sigma = 0.233.    11 better samples\n",
      "Episode 152\tAverage Score = -4.45, sigma = 0.231.     5 better samples\n",
      "Episode 153\tAverage Score = -4.43, sigma = 0.228.     3 better samples\n",
      "Episode 154\tAverage Score = -4.42, sigma = 0.226.     6 better samples\n",
      "Episode 155\tAverage Score = -4.42, sigma = 0.224.     3 better samples\n",
      "Episode 156\tAverage Score = -4.43, sigma = 0.221.    28 better samples\n",
      "Episode 157\tAverage Score = -4.49, sigma = 0.219.    22 better samples\n",
      "Episode 158\tAverage Score = -4.51, sigma = 0.217.    17 better samples\n",
      "Episode 159\tAverage Score = -4.52, sigma = 0.215.    17 better samples\n",
      "Episode 160\tAverage Score = -4.54, sigma = 0.213.    15 better samples\n",
      "Episode 161\tAverage Score = -4.60, sigma = 0.211.    23 better samples\n",
      "Episode 162\tAverage Score = -4.64, sigma = 0.208.    26 better samples\n",
      "Episode 163\tAverage Score = -4.66, sigma = 0.206.    11 better samples\n",
      "Episode 164\tAverage Score = -4.64, sigma = 0.204.   *** No better samples found.\n",
      "Episode 165\tAverage Score = -4.56, sigma = 0.202.    20 better samples\n",
      "Episode 166\tAverage Score = -4.58, sigma = 0.200.    23 better samples\n",
      "Episode 167\tAverage Score = -4.51, sigma = 0.198.    21 better samples\n",
      "Episode 168\tAverage Score = -4.44, sigma = 0.196.     1 better samples\n",
      "Episode 169\tAverage Score = -4.40, sigma = 0.194.     1 better samples\n",
      "Episode 170\tAverage Score = -4.25, sigma = 0.192.     1 better samples\n",
      "Episode 171\tAverage Score = -4.25, sigma = 0.190.    14 better samples\n",
      "Episode 172\tAverage Score = -4.21, sigma = 0.189.    10 better samples\n",
      "Episode 173\tAverage Score = -4.22, sigma = 0.187.    12 better samples\n",
      "Episode 174\tAverage Score = -4.17, sigma = 0.185.    31 better samples\n",
      "Episode 175\tAverage Score = -4.11, sigma = 0.183.     9 better samples\n",
      "Episode 176\tAverage Score = -4.13, sigma = 0.181.    21 better samples\n",
      "Episode 177\tAverage Score = -4.05, sigma = 0.179.     6 better samples\n",
      "Episode 178\tAverage Score = -4.02, sigma = 0.178.     4 better samples\n",
      "Episode 179\tAverage Score = -4.01, sigma = 0.176.     1 better samples\n",
      "Episode 180\tAverage Score = -3.95, sigma = 0.174.     1 better samples\n",
      "Episode 181\tAverage Score = -3.98, sigma = 0.172.    42 better samples\n",
      "Episode 182\tAverage Score = -3.99, sigma = 0.171.    23 better samples\n",
      "Episode 183\tAverage Score = -4.04, sigma = 0.169.    32 better samples\n",
      "Episode 184\tAverage Score = -3.94, sigma = 0.167.     6 better samples\n",
      "Episode 185\tAverage Score = -3.88, sigma = 0.165.    25 better samples\n",
      "Episode 186\tAverage Score = -3.89, sigma = 0.164.     4 better samples\n",
      "Episode 187\tAverage Score = -3.97, sigma = 0.162.    35 better samples\n",
      "Episode 188\tAverage Score = -3.97, sigma = 0.161.    11 better samples\n",
      "Episode 189\tAverage Score = -3.97, sigma = 0.159.     8 better samples\n",
      "Episode 190\tAverage Score = -3.94, sigma = 0.157.     6 better samples\n",
      "Episode 191\tAverage Score = -3.94, sigma = 0.156.     2 better samples\n",
      "Episode 192\tAverage Score = -3.93, sigma = 0.154.    25 better samples\n",
      "Episode 193\tAverage Score = -3.94, sigma = 0.153.    13 better samples\n",
      "Episode 194\tAverage Score = -3.89, sigma = 0.151.    14 better samples\n",
      "Episode 195\tAverage Score = -3.87, sigma = 0.150.    26 better samples\n",
      "Episode 196\tAverage Score = -3.84, sigma = 0.148.     8 better samples\n",
      "Episode 197\tAverage Score = -3.81, sigma = 0.147.   *** No better samples found.\n",
      "Episode 198\tAverage Score = -3.79, sigma = 0.145.    16 better samples\n",
      "Episode 199\tAverage Score = -3.74, sigma = 0.144.     6 better samples\n",
      "Episode 200\tAverage Score = -3.74, sigma = 0.142.    16 better samples\n",
      "Episode 201\tAverage Score = -3.66, sigma = 0.141.    14 better samples\n",
      "Episode 202\tAverage Score = -3.64, sigma = 0.139.    20 better samples\n",
      "Episode 203\tAverage Score = -3.69, sigma = 0.138.    25 better samples\n",
      "Episode 204\tAverage Score = -3.66, sigma = 0.137.    14 better samples\n",
      "Episode 205\tAverage Score = -3.73, sigma = 0.135.    48 better samples\n",
      "Episode 206\tAverage Score = -3.78, sigma = 0.134.    20 better samples\n",
      "Episode 207\tAverage Score = -3.82, sigma = 0.133.    28 better samples\n",
      "Episode 208\tAverage Score = -3.86, sigma = 0.131.    40 better samples\n",
      "Episode 209\tAverage Score = -3.86, sigma = 0.130.    15 better samples\n",
      "Episode 210\tAverage Score = -3.86, sigma = 0.129.    30 better samples\n",
      "Episode 211\tAverage Score = -3.86, sigma = 0.127.     8 better samples\n",
      "Episode 212\tAverage Score = -3.81, sigma = 0.126.    16 better samples\n",
      "Episode 213\tAverage Score = -3.84, sigma = 0.125.    22 better samples\n",
      "Episode 214\tAverage Score = -3.80, sigma = 0.124.     6 better samples\n",
      "Episode 215\tAverage Score = -3.82, sigma = 0.122.     6 better samples\n",
      "Episode 216\tAverage Score = -3.82, sigma = 0.121.     4 better samples\n",
      "Episode 217\tAverage Score = -3.86, sigma = 0.120.    30 better samples\n",
      "Episode 218\tAverage Score = -3.89, sigma = 0.119.    43 better samples\n",
      "Episode 219\tAverage Score = -3.95, sigma = 0.118.    27 better samples\n",
      "Episode 220\tAverage Score = -3.89, sigma = 0.116.    12 better samples\n",
      "Episode 221\tAverage Score = -3.86, sigma = 0.115.     8 better samples\n",
      "Episode 222\tAverage Score = -3.87, sigma = 0.114.    21 better samples\n",
      "Episode 223\tAverage Score = -3.91, sigma = 0.113.    19 better samples\n",
      "Episode 224\tAverage Score = -3.96, sigma = 0.112.    39 better samples\n",
      "Episode 225\tAverage Score = -3.96, sigma = 0.111.     6 better samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 226\tAverage Score = -4.03, sigma = 0.110.    42 better samples\n",
      "Episode 227\tAverage Score = -4.00, sigma = 0.108.   *** No better samples found.\n",
      "Episode 228\tAverage Score = -4.04, sigma = 0.107.    35 better samples\n",
      "Episode 229\tAverage Score = -4.02, sigma = 0.106.     1 better samples\n",
      "Episode 230\tAverage Score = -3.98, sigma = 0.105.    22 better samples\n",
      "Episode 231\tAverage Score = -3.97, sigma = 0.104.    29 better samples\n",
      "Episode 232\tAverage Score = -3.98, sigma = 0.103.    26 better samples\n",
      "Episode 233\tAverage Score = -4.05, sigma = 0.102.    42 better samples\n",
      "Episode 234\tAverage Score = -4.04, sigma = 0.101.    10 better samples\n",
      "Episode 235\tAverage Score = -4.05, sigma = 0.100.     4 better samples\n",
      "Episode 236\tAverage Score = -4.09, sigma = 0.099.    34 better samples\n",
      "Episode 237\tAverage Score = -4.09, sigma = 0.098.    18 better samples\n",
      "Episode 238\tAverage Score = -4.11, sigma = 0.097.    21 better samples\n",
      "Episode 239\tAverage Score = -4.13, sigma = 0.096.    18 better samples\n",
      "Episode 240\tAverage Score = -4.17, sigma = 0.095.    33 better samples\n",
      "Episode 241\tAverage Score = -4.20, sigma = 0.094.    13 better samples\n",
      "Episode 242\tAverage Score = -4.21, sigma = 0.093.     3 better samples\n",
      "Episode 243\tAverage Score = -4.19, sigma = 0.092.     1 better samples\n",
      "Episode 244\tAverage Score = -4.18, sigma = 0.091.     6 better samples\n",
      "Episode 245\tAverage Score = -4.19, sigma = 0.091.    14 better samples\n",
      "Episode 246\tAverage Score = -4.17, sigma = 0.090.    20 better samples\n",
      "Episode 247\tAverage Score = -4.16, sigma = 0.089.    14 better samples\n",
      "Episode 248\tAverage Score = -4.12, sigma = 0.088.    30 better samples\n",
      "Episode 249\tAverage Score = -4.09, sigma = 0.087.    10 better samples\n",
      "Episode 250\tAverage Score = -4.05, sigma = 0.086.     3 better samples\n",
      "Episode 251\tAverage Score = -4.08, sigma = 0.085.    27 better samples\n",
      "Episode 252\tAverage Score = -4.15, sigma = 0.084.    42 better samples\n",
      "Episode 253\tAverage Score = -4.26, sigma = 0.084.    48 better samples\n",
      "Episode 254\tAverage Score = -4.26, sigma = 0.083.    12 better samples\n",
      "Episode 255\tAverage Score = -4.30, sigma = 0.082.    30 better samples\n",
      "Episode 256\tAverage Score = -4.26, sigma = 0.081.    13 better samples\n",
      "Episode 257\tAverage Score = -4.21, sigma = 0.080.    14 better samples\n",
      "Episode 258\tAverage Score = -4.18, sigma = 0.079.     2 better samples\n",
      "Episode 259\tAverage Score = -4.14, sigma = 0.079.     3 better samples\n",
      "Episode 260\tAverage Score = -4.10, sigma = 0.078.     1 better samples\n",
      "Episode 261\tAverage Score = -4.05, sigma = 0.077.     1 better samples\n",
      "Episode 262\tAverage Score = -4.09, sigma = 0.076.    46 better samples\n",
      "Episode 263\tAverage Score = -4.06, sigma = 0.076.     1 better samples\n",
      "Episode 264\tAverage Score = -4.11, sigma = 0.075.    28 better samples\n",
      "Episode 265\tAverage Score = -4.09, sigma = 0.074.    12 better samples\n",
      "Episode 266\tAverage Score = -4.08, sigma = 0.073.    11 better samples\n",
      "Episode 267\tAverage Score = -4.17, sigma = 0.073.    48 better samples\n",
      "Episode 268\tAverage Score = -4.20, sigma = 0.072.    19 better samples\n",
      "Episode 269\tAverage Score = -4.21, sigma = 0.071.    10 better samples\n",
      "Episode 270\tAverage Score = -4.31, sigma = 0.070.    40 better samples\n",
      "Episode 271\tAverage Score = -4.31, sigma = 0.070.    20 better samples\n",
      "Episode 272\tAverage Score = -4.33, sigma = 0.069.    28 better samples\n",
      "Episode 273\tAverage Score = -4.31, sigma = 0.068.     5 better samples\n",
      "Episode 274\tAverage Score = -4.27, sigma = 0.068.    18 better samples\n",
      "Episode 275\tAverage Score = -4.34, sigma = 0.067.    36 better samples\n",
      "Episode 276\tAverage Score = -4.39, sigma = 0.066.    41 better samples\n",
      "Episode 277\tAverage Score = -4.41, sigma = 0.066.    28 better samples\n",
      "Episode 278\tAverage Score = -4.40, sigma = 0.065.     2 better samples\n",
      "Episode 279\tAverage Score = -4.45, sigma = 0.064.    22 better samples\n",
      "Episode 280\tAverage Score = -4.45, sigma = 0.064.    10 better samples\n",
      "Episode 281\tAverage Score = -4.42, sigma = 0.063.    41 better samples\n",
      "Episode 282\tAverage Score = -4.40, sigma = 0.062.    23 better samples\n",
      "Episode 283\tAverage Score = -4.40, sigma = 0.062.    29 better samples\n",
      "Episode 284\tAverage Score = -4.43, sigma = 0.061.    23 better samples\n",
      "Episode 285\tAverage Score = -4.37, sigma = 0.061.     4 better samples\n",
      "Episode 286\tAverage Score = -4.38, sigma = 0.060.    16 better samples\n",
      "Episode 287\tAverage Score = -4.37, sigma = 0.059.    37 better samples\n",
      "Episode 288\tAverage Score = -4.39, sigma = 0.059.    20 better samples\n",
      "Episode 289\tAverage Score = -4.44, sigma = 0.058.    33 better samples\n",
      "Episode 290\tAverage Score = -4.45, sigma = 0.058.    10 better samples\n",
      "Episode 291\tAverage Score = -4.55, sigma = 0.057.    45 better samples\n",
      "Episode 292\tAverage Score = -4.53, sigma = 0.056.    17 better samples\n",
      "Episode 293\tAverage Score = -4.58, sigma = 0.056.    37 better samples\n",
      "Episode 294\tAverage Score = -4.56, sigma = 0.055.     8 better samples\n",
      "Episode 295\tAverage Score = -4.53, sigma = 0.055.    15 better samples\n",
      "Episode 296\tAverage Score = -4.54, sigma = 0.054.    15 better samples\n",
      "Episode 297\tAverage Score = -4.58, sigma = 0.054.    20 better samples\n",
      "Episode 298\tAverage Score = -4.59, sigma = 0.053.    19 better samples\n",
      "Episode 299\tAverage Score = -4.64, sigma = 0.053.    28 better samples\n",
      "Episode 300\tAverage Score = -4.66, sigma = 0.052.    37 better samples\n",
      "Episode 301\tAverage Score = -4.67, sigma = 0.052.    21 better samples\n",
      "Episode 302\tAverage Score = -4.68, sigma = 0.051.    23 better samples\n",
      "Episode 303\tAverage Score = -4.68, sigma = 0.051.    38 better samples\n",
      "Episode 304\tAverage Score = -4.71, sigma = 0.050.    29 better samples\n",
      "Episode 305\tAverage Score = -4.65, sigma = 0.050.    23 better samples\n",
      "Episode 306\tAverage Score = -4.64, sigma = 0.049.    18 better samples\n",
      "Episode 307\tAverage Score = -4.67, sigma = 0.049.    45 better samples\n",
      "Episode 308\tAverage Score = -4.67, sigma = 0.048.    39 better samples\n",
      "Episode 309\tAverage Score = -4.70, sigma = 0.048.    31 better samples\n",
      "Episode 310\tAverage Score = -4.64, sigma = 0.047.    12 better samples\n",
      "Episode 311\tAverage Score = -4.72, sigma = 0.047.    44 better samples\n",
      "Episode 312\tAverage Score = -4.79, sigma = 0.046.    40 better samples\n",
      "Episode 313\tAverage Score = -4.82, sigma = 0.046.    30 better samples\n",
      "Episode 314\tAverage Score = -4.94, sigma = 0.045.    49 better samples\n",
      "Episode 315\tAverage Score = -4.97, sigma = 0.045.    26 better samples\n",
      "Episode 316\tAverage Score = -4.98, sigma = 0.044.    10 better samples\n",
      "Episode 317\tAverage Score = -5.06, sigma = 0.044.    50 better samples\n",
      "Episode 318\tAverage Score = -5.03, sigma = 0.043.    31 better samples\n",
      "Episode 319\tAverage Score = -5.02, sigma = 0.043.    21 better samples\n",
      "Episode 320\tAverage Score = -5.05, sigma = 0.043.    19 better samples\n",
      "Episode 321\tAverage Score = -5.11, sigma = 0.042.    26 better samples\n",
      "Episode 322\tAverage Score = -5.16, sigma = 0.042.    37 better samples\n",
      "Episode 323\tAverage Score = -5.18, sigma = 0.041.    26 better samples\n",
      "Episode 324\tAverage Score = -5.25, sigma = 0.041.    50 better samples\n",
      "Episode 325\tAverage Score = -5.25, sigma = 0.041.     2 better samples\n",
      "Episode 326\tAverage Score = -5.16, sigma = 0.040.     1 better samples\n",
      "Episode 327\tAverage Score = -5.23, sigma = 0.040.    28 better samples\n",
      "Episode 328\tAverage Score = -5.19, sigma = 0.039.    10 better samples\n",
      "Episode 329\tAverage Score = -5.29, sigma = 0.039.    44 better samples\n",
      "Episode 330\tAverage Score = -5.28, sigma = 0.039.    10 better samples\n",
      "Episode 331\tAverage Score = -5.28, sigma = 0.038.    21 better samples\n",
      "Episode 332\tAverage Score = -5.24, sigma = 0.038.     4 better samples\n",
      "Episode 333\tAverage Score = -5.18, sigma = 0.037.    13 better samples\n",
      "Episode 334\tAverage Score = -5.23, sigma = 0.037.    41 better samples\n",
      "Episode 335\tAverage Score = -5.26, sigma = 0.037.    20 better samples\n",
      "Episode 336\tAverage Score = -5.24, sigma = 0.036.    15 better samples\n",
      "Episode 337\tAverage Score = -5.22, sigma = 0.036.     7 better samples\n",
      "Episode 338\tAverage Score = -5.25, sigma = 0.036.    34 better samples\n",
      "Episode 339\tAverage Score = -5.27, sigma = 0.035.    20 better samples\n",
      "Episode 340\tAverage Score = -5.30, sigma = 0.035.    44 better samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 341\tAverage Score = -5.31, sigma = 0.034.    14 better samples\n",
      "Episode 342\tAverage Score = -5.39, sigma = 0.034.    44 better samples\n",
      "Episode 343\tAverage Score = -5.46, sigma = 0.034.    35 better samples\n",
      "Episode 344\tAverage Score = -5.47, sigma = 0.033.    12 better samples\n",
      "Episode 345\tAverage Score = -5.51, sigma = 0.033.    28 better samples\n",
      "Episode 346\tAverage Score = -5.53, sigma = 0.033.    26 better samples\n",
      "Episode 347\tAverage Score = -5.61, sigma = 0.032.    46 better samples\n",
      "Episode 348\tAverage Score = -5.63, sigma = 0.032.    25 better samples\n",
      "Episode 349\tAverage Score = -5.66, sigma = 0.032.    13 better samples\n",
      "Episode 350\tAverage Score = -5.76, sigma = 0.032.    41 better samples\n",
      "Episode 351\tAverage Score = -5.75, sigma = 0.031.    20 better samples\n",
      "Episode 352\tAverage Score = -5.72, sigma = 0.031.    29 better samples\n",
      "Episode 353\tAverage Score = -5.66, sigma = 0.031.    24 better samples\n",
      "Episode 354\tAverage Score = -5.65, sigma = 0.030.     5 better samples\n",
      "Episode 355\tAverage Score = -5.62, sigma = 0.030.    11 better samples\n",
      "Episode 356\tAverage Score = -5.70, sigma = 0.030.    44 better samples\n",
      "Episode 357\tAverage Score = -5.75, sigma = 0.029.    33 better samples\n",
      "Episode 358\tAverage Score = -5.78, sigma = 0.029.    13 better samples\n",
      "Episode 359\tAverage Score = -5.79, sigma = 0.029.     2 better samples\n",
      "Episode 360\tAverage Score = -5.84, sigma = 0.029.    19 better samples\n",
      "Episode 361\tAverage Score = -5.93, sigma = 0.028.    42 better samples\n",
      "Episode 362\tAverage Score = -5.92, sigma = 0.028.    46 better samples\n",
      "Episode 363\tAverage Score = -6.02, sigma = 0.028.    47 better samples\n",
      "Episode 364\tAverage Score = -6.03, sigma = 0.027.    33 better samples\n",
      "Episode 365\tAverage Score = -6.06, sigma = 0.027.    26 better samples\n",
      "Episode 366\tAverage Score = -6.07, sigma = 0.027.    21 better samples\n",
      "Episode 367\tAverage Score = -5.99, sigma = 0.027.    22 better samples\n",
      "Episode 368\tAverage Score = -6.00, sigma = 0.026.    17 better samples\n",
      "Episode 369\tAverage Score = -6.07, sigma = 0.026.    41 better samples\n",
      "Episode 370\tAverage Score = -5.99, sigma = 0.026.     6 better samples\n",
      "Episode 371\tAverage Score = -6.01, sigma = 0.026.    27 better samples\n",
      "Episode 372\tAverage Score = -6.01, sigma = 0.025.    29 better samples\n",
      "Episode 373\tAverage Score = -6.05, sigma = 0.025.    21 better samples\n",
      "Episode 374\tAverage Score = -6.04, sigma = 0.025.     2 better samples\n",
      "Episode 375\tAverage Score = -5.99, sigma = 0.025.    28 better samples\n",
      "Episode 376\tAverage Score = -5.94, sigma = 0.024.    25 better samples\n",
      "Episode 377\tAverage Score = -5.96, sigma = 0.024.    29 better samples\n",
      "Episode 378\tAverage Score = -6.02, sigma = 0.024.    31 better samples\n",
      "Episode 379\tAverage Score = -6.04, sigma = 0.024.    33 better samples\n",
      "Episode 380\tAverage Score = -6.07, sigma = 0.023.    22 better samples\n",
      "Episode 381\tAverage Score = -6.07, sigma = 0.023.    41 better samples\n",
      "Episode 382\tAverage Score = -6.07, sigma = 0.023.    20 better samples\n",
      "Episode 383\tAverage Score = -6.08, sigma = 0.023.    37 better samples\n",
      "Episode 384\tAverage Score = -6.16, sigma = 0.022.    50 better samples\n",
      "Episode 385\tAverage Score = -6.16, sigma = 0.022.     1 better samples\n",
      "Episode 386\tAverage Score = -6.19, sigma = 0.022.    24 better samples\n",
      "Episode 387\tAverage Score = -6.21, sigma = 0.022.    42 better samples\n",
      "Episode 388\tAverage Score = -6.19, sigma = 0.022.    14 better samples\n",
      "Episode 389\tAverage Score = -6.14, sigma = 0.021.     8 better samples\n",
      "Episode 390\tAverage Score = -6.17, sigma = 0.021.    23 better samples\n",
      "Episode 391\tAverage Score = -6.14, sigma = 0.021.    44 better samples\n",
      "Episode 392\tAverage Score = -6.11, sigma = 0.021.     3 better samples\n",
      "Episode 393\tAverage Score = -6.16, sigma = 0.020.    49 better samples\n",
      "Episode 394\tAverage Score = -6.15, sigma = 0.020.     5 better samples\n",
      "Episode 395\tAverage Score = -6.15, sigma = 0.020.    15 better samples\n",
      "Episode 396\tAverage Score = -6.15, sigma = 0.020.    18 better samples\n",
      "Episode 397\tAverage Score = -6.18, sigma = 0.020.    35 better samples\n",
      "Episode 398\tAverage Score = -6.15, sigma = 0.019.     9 better samples\n",
      "Episode 399\tAverage Score = -6.15, sigma = 0.019.    26 better samples\n",
      "Episode 400\tAverage Score = -6.14, sigma = 0.019.    36 better samples\n",
      "Episode 401\tAverage Score = -6.18, sigma = 0.019.    42 better samples\n",
      "Episode 402\tAverage Score = -6.17, sigma = 0.019.    18 better samples\n",
      "Episode 403\tAverage Score = -6.12, sigma = 0.019.    12 better samples\n",
      "Episode 404\tAverage Score = -6.11, sigma = 0.018.    16 better samples\n",
      "Episode 405\tAverage Score = -6.06, sigma = 0.018.     5 better samples\n",
      "Episode 406\tAverage Score = -6.06, sigma = 0.018.    31 better samples\n",
      "Episode 407\tAverage Score = -6.03, sigma = 0.018.    37 better samples\n",
      "Episode 408\tAverage Score = -6.04, sigma = 0.018.    46 better samples\n",
      "Episode 409\tAverage Score = -6.05, sigma = 0.017.    30 better samples\n",
      "Episode 410\tAverage Score = -6.04, sigma = 0.017.     6 better samples\n",
      "Episode 411\tAverage Score = -6.05, sigma = 0.017.    49 better samples\n",
      "Episode 412\tAverage Score = -6.02, sigma = 0.017.    35 better samples\n",
      "Episode 413\tAverage Score = -6.02, sigma = 0.017.    30 better samples\n",
      "Episode 414\tAverage Score = -5.96, sigma = 0.017.    40 better samples\n",
      "Episode 415\tAverage Score = -6.02, sigma = 0.016.    48 better samples\n",
      "Episode 416\tAverage Score = -6.05, sigma = 0.016.    27 better samples\n",
      "Episode 417\tAverage Score = -5.95, sigma = 0.016.    22 better samples\n",
      "Episode 418\tAverage Score = -5.95, sigma = 0.016.    38 better samples\n",
      "Episode 419\tAverage Score = -5.94, sigma = 0.016.    20 better samples\n",
      "Episode 420\tAverage Score = -5.95, sigma = 0.016.    27 better samples\n",
      "Episode 421\tAverage Score = -5.94, sigma = 0.015.    26 better samples\n",
      "Episode 422\tAverage Score = -5.93, sigma = 0.015.    33 better samples\n",
      "Episode 423\tAverage Score = -5.90, sigma = 0.015.    22 better samples\n",
      "Episode 424\tAverage Score = -5.78, sigma = 0.015.     4 better samples\n",
      "Episode 425\tAverage Score = -5.85, sigma = 0.015.    42 better samples\n",
      "Episode 426\tAverage Score = -5.91, sigma = 0.015.    35 better samples\n",
      "Episode 427\tAverage Score = -5.85, sigma = 0.015.     2 better samples\n",
      "Episode 428\tAverage Score = -5.88, sigma = 0.014.    36 better samples\n",
      "Episode 429\tAverage Score = -5.82, sigma = 0.014.    20 better samples\n",
      "Episode 430\tAverage Score = -5.87, sigma = 0.014.    37 better samples\n",
      "Episode 431\tAverage Score = -5.93, sigma = 0.014.    45 better samples\n",
      "Episode 432\tAverage Score = -5.95, sigma = 0.014.    21 better samples\n",
      "Episode 433\tAverage Score = -5.98, sigma = 0.014.    32 better samples\n",
      "Episode 434\tAverage Score = -5.94, sigma = 0.014.    24 better samples\n",
      "Episode 435\tAverage Score = -5.93, sigma = 0.013.    18 better samples\n",
      "Episode 436\tAverage Score = -5.89, sigma = 0.013.   *** No better samples found.\n",
      "Episode 437\tAverage Score = -5.90, sigma = 0.013.    26 better samples\n",
      "Episode 438\tAverage Score = -5.88, sigma = 0.013.    29 better samples\n",
      "Episode 439\tAverage Score = -5.83, sigma = 0.013.   *** No better samples found.\n",
      "Episode 440\tAverage Score = -5.78, sigma = 0.013.    27 better samples\n",
      "Episode 441\tAverage Score = -5.76, sigma = 0.013.    13 better samples\n",
      "Episode 442\tAverage Score = -5.68, sigma = 0.013.     5 better samples\n",
      "Episode 443\tAverage Score = -5.62, sigma = 0.012.     9 better samples\n",
      "Episode 444\tAverage Score = -5.68, sigma = 0.012.    41 better samples\n",
      "Episode 445\tAverage Score = -5.66, sigma = 0.012.    33 better samples\n",
      "Episode 446\tAverage Score = -5.60, sigma = 0.012.     2 better samples\n",
      "Episode 447\tAverage Score = -5.60, sigma = 0.012.    44 better samples\n",
      "Episode 448\tAverage Score = -5.58, sigma = 0.012.    27 better samples\n",
      "Episode 449\tAverage Score = -5.62, sigma = 0.012.    39 better samples\n",
      "Episode 450\tAverage Score = -5.55, sigma = 0.012.    13 better samples\n",
      "Episode 451\tAverage Score = -5.52, sigma = 0.011.    12 better samples\n",
      "Episode 452\tAverage Score = -5.56, sigma = 0.011.    43 better samples\n",
      "Episode 453\tAverage Score = -5.56, sigma = 0.011.    33 better samples\n",
      "Episode 454\tAverage Score = -5.61, sigma = 0.011.    31 better samples\n",
      "Episode 455\tAverage Score = -5.63, sigma = 0.011.    27 better samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 456\tAverage Score = -5.56, sigma = 0.011.    17 better samples\n",
      "Episode 457\tAverage Score = -5.59, sigma = 0.011.    45 better samples\n",
      "Episode 458\tAverage Score = -5.65, sigma = 0.011.    44 better samples\n",
      "Episode 459\tAverage Score = -5.66, sigma = 0.011.    16 better samples\n",
      "Episode 460\tAverage Score = -5.65, sigma = 0.010.    16 better samples\n",
      "Episode 461\tAverage Score = -5.63, sigma = 0.010.    37 better samples\n",
      "Episode 462\tAverage Score = -5.61, sigma = 0.010.    37 better samples\n",
      "Episode 463\tAverage Score = -5.59, sigma = 0.010.    41 better samples\n",
      "Episode 464\tAverage Score = -5.59, sigma = 0.010.    32 better samples\n",
      "Episode 465\tAverage Score = -5.65, sigma = 0.010.    50 better samples\n",
      "Episode 466\tAverage Score = -5.61, sigma = 0.010.     2 better samples\n",
      "Episode 467\tAverage Score = -5.62, sigma = 0.010.    31 better samples\n",
      "Episode 468\tAverage Score = -5.58, sigma = 0.010.     3 better samples\n",
      "Episode 469\tAverage Score = -5.53, sigma = 0.010.    23 better samples\n",
      "Episode 470\tAverage Score = -5.60, sigma = 0.009.    35 better samples\n",
      "Episode 471\tAverage Score = -5.56, sigma = 0.009.    15 better samples\n",
      "Episode 472\tAverage Score = -5.52, sigma = 0.009.     1 better samples\n",
      "Episode 473\tAverage Score = -5.51, sigma = 0.009.    23 better samples\n",
      "Episode 474\tAverage Score = -5.56, sigma = 0.009.    28 better samples\n",
      "Episode 475\tAverage Score = -5.59, sigma = 0.009.    36 better samples\n",
      "Episode 476\tAverage Score = -5.64, sigma = 0.009.    45 better samples\n",
      "Episode 477\tAverage Score = -5.59, sigma = 0.009.    12 better samples\n",
      "Episode 478\tAverage Score = -5.58, sigma = 0.009.    28 better samples\n",
      "Episode 479\tAverage Score = -5.57, sigma = 0.009.    29 better samples\n",
      "Episode 480\tAverage Score = -5.57, sigma = 0.009.    18 better samples\n",
      "Episode 481\tAverage Score = -5.56, sigma = 0.008.    37 better samples\n",
      "Episode 482\tAverage Score = -5.58, sigma = 0.008.    30 better samples\n",
      "Episode 483\tAverage Score = -5.51, sigma = 0.008.     2 better samples\n",
      "Episode 484\tAverage Score = -5.38, sigma = 0.008.     3 better samples\n",
      "Episode 485\tAverage Score = -5.38, sigma = 0.008.     3 better samples\n",
      "Episode 486\tAverage Score = -5.34, sigma = 0.008.     3 better samples\n",
      "Episode 487\tAverage Score = -5.25, sigma = 0.008.     4 better samples\n",
      "Episode 488\tAverage Score = -5.24, sigma = 0.008.    14 better samples\n",
      "Episode 489\tAverage Score = -5.35, sigma = 0.008.    50 better samples\n",
      "Episode 490\tAverage Score = -5.39, sigma = 0.008.    41 better samples\n",
      "Episode 491\tAverage Score = -5.32, sigma = 0.008.    14 better samples\n",
      "Episode 492\tAverage Score = -5.32, sigma = 0.008.   *** No better samples found.\n",
      "Episode 493\tAverage Score = -5.22, sigma = 0.007.    12 better samples\n",
      "Episode 494\tAverage Score = -5.33, sigma = 0.007.    48 better samples\n",
      "Episode 495\tAverage Score = -5.37, sigma = 0.007.    37 better samples\n",
      "Episode 496\tAverage Score = -5.37, sigma = 0.007.    14 better samples\n",
      "Episode 497\tAverage Score = -5.43, sigma = 0.007.    50 better samples\n",
      "Episode 498\tAverage Score = -5.43, sigma = 0.007.     6 better samples\n",
      "Episode 499\tAverage Score = -5.37, sigma = 0.007.     6 better samples\n",
      "Episode 500\tAverage Score = -5.34, sigma = 0.007.    17 better samples\n",
      "Episode 501\tAverage Score = -5.30, sigma = 0.007.    25 better samples\n",
      "Episode 502\tAverage Score = -5.38, sigma = 0.007.    47 better samples\n",
      "Episode 503\tAverage Score = -5.36, sigma = 0.007.     1 better samples\n",
      "Episode 504\tAverage Score = -5.34, sigma = 0.007.    18 better samples\n",
      "Episode 505\tAverage Score = -5.39, sigma = 0.007.    31 better samples\n",
      "Episode 506\tAverage Score = -5.41, sigma = 0.007.    34 better samples\n",
      "Episode 507\tAverage Score = -5.41, sigma = 0.007.    30 better samples\n",
      "Episode 508\tAverage Score = -5.43, sigma = 0.006.    47 better samples\n",
      "Episode 509\tAverage Score = -5.41, sigma = 0.006.    21 better samples\n",
      "Episode 510\tAverage Score = -5.41, sigma = 0.006.     8 better samples\n",
      "Episode 511\tAverage Score = -5.35, sigma = 0.006.    25 better samples\n",
      "Episode 512\tAverage Score = -5.31, sigma = 0.006.    26 better samples\n",
      "Episode 513\tAverage Score = -5.33, sigma = 0.006.    40 better samples\n",
      "Episode 514\tAverage Score = -5.34, sigma = 0.006.    41 better samples\n",
      "Episode 515\tAverage Score = -5.29, sigma = 0.006.    31 better samples\n",
      "Episode 516\tAverage Score = -5.33, sigma = 0.006.    42 better samples\n",
      "Episode 517\tAverage Score = -5.29, sigma = 0.006.     4 better samples\n",
      "Episode 518\tAverage Score = -5.24, sigma = 0.006.    13 better samples\n",
      "Episode 519\tAverage Score = -5.28, sigma = 0.006.    41 better samples\n",
      "Episode 520\tAverage Score = -5.34, sigma = 0.006.    49 better samples\n",
      "Episode 521\tAverage Score = -5.30, sigma = 0.006.     8 better samples\n",
      "Episode 522\tAverage Score = -5.22, sigma = 0.006.     1 better samples\n",
      "Episode 523\tAverage Score = -5.20, sigma = 0.006.     8 better samples\n",
      "Episode 524\tAverage Score = -5.22, sigma = 0.005.    19 better samples\n",
      "Episode 525\tAverage Score = -5.20, sigma = 0.005.    38 better samples\n",
      "Episode 526\tAverage Score = -5.21, sigma = 0.005.    32 better samples\n",
      "Episode 527\tAverage Score = -5.27, sigma = 0.005.    33 better samples\n",
      "Episode 528\tAverage Score = -5.25, sigma = 0.005.    21 better samples\n",
      "Episode 529\tAverage Score = -5.27, sigma = 0.005.    28 better samples\n",
      "Episode 530\tAverage Score = -5.26, sigma = 0.005.    30 better samples\n",
      "Episode 531\tAverage Score = -5.24, sigma = 0.005.    42 better samples\n",
      "Episode 532\tAverage Score = -5.31, sigma = 0.005.    46 better samples\n",
      "Episode 533\tAverage Score = -5.31, sigma = 0.005.    33 better samples\n",
      "Episode 534\tAverage Score = -5.38, sigma = 0.005.    46 better samples\n",
      "Episode 535\tAverage Score = -5.44, sigma = 0.005.    41 better samples\n",
      "Episode 536\tAverage Score = -5.46, sigma = 0.005.    16 better samples\n",
      "Episode 537\tAverage Score = -5.48, sigma = 0.005.    25 better samples\n",
      "Episode 538\tAverage Score = -5.44, sigma = 0.005.     7 better samples\n",
      "Episode 539\tAverage Score = -5.51, sigma = 0.005.    33 better samples\n",
      "Episode 540\tAverage Score = -5.49, sigma = 0.005.    17 better samples\n",
      "Episode 541\tAverage Score = -5.50, sigma = 0.005.    25 better samples\n",
      "Episode 542\tAverage Score = -5.52, sigma = 0.005.    14 better samples\n",
      "Episode 543\tAverage Score = -5.55, sigma = 0.005.    28 better samples\n",
      "Episode 544\tAverage Score = -5.48, sigma = 0.004.     6 better samples\n",
      "Episode 545\tAverage Score = -5.44, sigma = 0.004.     9 better samples\n",
      "Episode 546\tAverage Score = -5.51, sigma = 0.004.    28 better samples\n",
      "Episode 547\tAverage Score = -5.47, sigma = 0.004.    40 better samples\n",
      "Episode 548\tAverage Score = -5.42, sigma = 0.004.     3 better samples\n",
      "Episode 549\tAverage Score = -5.34, sigma = 0.004.     4 better samples\n",
      "Episode 550\tAverage Score = -5.35, sigma = 0.004.    19 better samples\n",
      "Episode 551\tAverage Score = -5.38, sigma = 0.004.    28 better samples\n",
      "Episode 552\tAverage Score = -5.32, sigma = 0.004.    28 better samples\n",
      "Episode 553\tAverage Score = -5.28, sigma = 0.004.    16 better samples\n",
      "Episode 554\tAverage Score = -5.30, sigma = 0.004.    40 better samples\n",
      "Episode 555\tAverage Score = -5.27, sigma = 0.004.    13 better samples\n",
      "Episode 556\tAverage Score = -5.26, sigma = 0.004.    12 better samples\n",
      "Episode 557\tAverage Score = -5.18, sigma = 0.004.    11 better samples\n",
      "Episode 558\tAverage Score = -5.17, sigma = 0.004.    40 better samples\n",
      "Episode 559\tAverage Score = -5.26, sigma = 0.004.    50 better samples\n",
      "Episode 560\tAverage Score = -5.28, sigma = 0.004.    27 better samples\n",
      "Episode 561\tAverage Score = -5.21, sigma = 0.004.     8 better samples\n",
      "Episode 562\tAverage Score = -5.21, sigma = 0.004.    42 better samples\n",
      "Episode 563\tAverage Score = -5.14, sigma = 0.004.    10 better samples\n",
      "Episode 564\tAverage Score = -5.12, sigma = 0.004.    21 better samples\n",
      "Episode 565\tAverage Score = -5.09, sigma = 0.004.    43 better samples\n",
      "Episode 566\tAverage Score = -5.12, sigma = 0.004.    18 better samples\n",
      "Episode 567\tAverage Score = -5.17, sigma = 0.004.    44 better samples\n",
      "Episode 568\tAverage Score = -5.22, sigma = 0.004.    29 better samples\n",
      "Episode 569\tAverage Score = -5.22, sigma = 0.003.    23 better samples\n",
      "Episode 570\tAverage Score = -5.16, sigma = 0.003.    15 better samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 571\tAverage Score = -5.21, sigma = 0.003.    34 better samples\n",
      "Episode 572\tAverage Score = -5.27, sigma = 0.003.    31 better samples\n",
      "Episode 573\tAverage Score = -5.30, sigma = 0.003.    37 better samples\n",
      "Episode 574\tAverage Score = -5.27, sigma = 0.003.    12 better samples\n",
      "Episode 575\tAverage Score = -5.28, sigma = 0.003.    41 better samples\n",
      "Episode 576\tAverage Score = -5.26, sigma = 0.003.    43 better samples\n",
      "Episode 577\tAverage Score = -5.25, sigma = 0.003.    14 better samples\n",
      "Episode 578\tAverage Score = -5.29, sigma = 0.003.    42 better samples\n",
      "Episode 579\tAverage Score = -5.24, sigma = 0.003.     1 better samples\n",
      "Episode 580\tAverage Score = -5.22, sigma = 0.003.     9 better samples\n",
      "Episode 581\tAverage Score = -5.16, sigma = 0.003.     7 better samples\n",
      "Episode 582\tAverage Score = -5.17, sigma = 0.003.    33 better samples\n",
      "Episode 583\tAverage Score = -5.25, sigma = 0.003.    41 better samples\n",
      "Episode 584\tAverage Score = -5.24, sigma = 0.003.     2 better samples\n",
      "Episode 585\tAverage Score = -5.32, sigma = 0.003.    41 better samples\n",
      "Episode 586\tAverage Score = -5.43, sigma = 0.003.    49 better samples\n",
      "Episode 587\tAverage Score = -5.43, sigma = 0.003.     3 better samples\n",
      "Episode 588\tAverage Score = -5.53, sigma = 0.003.    50 better samples\n",
      "Episode 589\tAverage Score = -5.41, sigma = 0.003.     6 better samples\n",
      "Episode 590\tAverage Score = -5.39, sigma = 0.003.    36 better samples\n",
      "Episode 591\tAverage Score = -5.40, sigma = 0.003.    13 better samples\n",
      "Episode 592\tAverage Score = -5.51, sigma = 0.003.    49 better samples\n",
      "Episode 593\tAverage Score = -5.54, sigma = 0.003.    31 better samples\n",
      "Episode 594\tAverage Score = -5.49, sigma = 0.003.    33 better samples\n",
      "Episode 595\tAverage Score = -5.43, sigma = 0.003.    13 better samples\n",
      "Episode 596\tAverage Score = -5.39, sigma = 0.003.     1 better samples\n",
      "Episode 597\tAverage Score = -5.35, sigma = 0.003.    38 better samples\n",
      "Episode 598\tAverage Score = -5.37, sigma = 0.003.    14 better samples\n",
      "Episode 599\tAverage Score = -5.47, sigma = 0.003.    47 better samples\n",
      "Episode 600\tAverage Score = -5.48, sigma = 0.003.    21 better samples\n",
      "Episode 601\tAverage Score = -5.46, sigma = 0.003.    22 better samples\n",
      "Episode 602\tAverage Score = -5.40, sigma = 0.003.    26 better samples\n",
      "Episode 603\tAverage Score = -5.40, sigma = 0.002.     1 better samples\n",
      "Episode 604\tAverage Score = -5.39, sigma = 0.002.    11 better samples\n",
      "Episode 605\tAverage Score = -5.37, sigma = 0.002.    19 better samples\n",
      "Episode 606\tAverage Score = -5.34, sigma = 0.002.    27 better samples\n",
      "Episode 607\tAverage Score = -5.34, sigma = 0.002.    33 better samples\n",
      "Episode 608\tAverage Score = -5.31, sigma = 0.002.    41 better samples\n",
      "Episode 609\tAverage Score = -5.27, sigma = 0.002.     2 better samples\n",
      "Episode 610\tAverage Score = -5.27, sigma = 0.002.    10 better samples\n",
      "Episode 611\tAverage Score = -5.24, sigma = 0.002.    12 better samples\n",
      "Episode 612\tAverage Score = -5.22, sigma = 0.002.    16 better samples\n",
      "Episode 613\tAverage Score = -5.19, sigma = 0.002.    33 better samples\n",
      "Episode 614\tAverage Score = -5.23, sigma = 0.002.    44 better samples\n",
      "Episode 615\tAverage Score = -5.19, sigma = 0.002.    10 better samples\n",
      "Episode 616\tAverage Score = -5.18, sigma = 0.002.    39 better samples\n",
      "Episode 617\tAverage Score = -5.21, sigma = 0.002.    27 better samples\n",
      "Episode 618\tAverage Score = -5.25, sigma = 0.002.    32 better samples\n",
      "Episode 619\tAverage Score = -5.19, sigma = 0.002.    16 better samples\n",
      "Episode 620\tAverage Score = -5.13, sigma = 0.002.    33 better samples\n",
      "Episode 621\tAverage Score = -5.20, sigma = 0.002.    40 better samples\n",
      "Episode 622\tAverage Score = -5.20, sigma = 0.002.     4 better samples\n",
      "Episode 623\tAverage Score = -5.31, sigma = 0.002.    48 better samples\n",
      "Episode 624\tAverage Score = -5.40, sigma = 0.002.    50 better samples\n",
      "Episode 625\tAverage Score = -5.34, sigma = 0.002.     7 better samples\n",
      "Episode 626\tAverage Score = -5.38, sigma = 0.002.    46 better samples\n",
      "Episode 627\tAverage Score = -5.32, sigma = 0.002.     4 better samples\n",
      "Episode 628\tAverage Score = -5.34, sigma = 0.002.    31 better samples\n",
      "Episode 629\tAverage Score = -5.36, sigma = 0.002.    43 better samples\n",
      "Episode 630\tAverage Score = -5.40, sigma = 0.002.    48 better samples\n",
      "Episode 631\tAverage Score = -5.36, sigma = 0.002.    26 better samples\n",
      "Episode 632\tAverage Score = -5.29, sigma = 0.002.    20 better samples\n",
      "Episode 633\tAverage Score = -5.26, sigma = 0.002.    21 better samples\n",
      "Episode 634\tAverage Score = -5.21, sigma = 0.002.    17 better samples\n",
      "Episode 635\tAverage Score = -5.11, sigma = 0.002.     1 better samples\n",
      "Episode 636\tAverage Score = -5.14, sigma = 0.002.    28 better samples\n",
      "Episode 637\tAverage Score = -5.10, sigma = 0.002.     4 better samples\n",
      "Episode 638\tAverage Score = -5.11, sigma = 0.002.    19 better samples\n",
      "Episode 639\tAverage Score = -5.07, sigma = 0.002.    22 better samples\n",
      "Episode 640\tAverage Score = -5.11, sigma = 0.002.    31 better samples\n",
      "Episode 641\tAverage Score = -5.08, sigma = 0.002.     3 better samples\n",
      "Episode 642\tAverage Score = -5.10, sigma = 0.002.    23 better samples\n",
      "Episode 643\tAverage Score = -5.10, sigma = 0.002.    24 better samples\n",
      "Episode 644\tAverage Score = -5.13, sigma = 0.002.    28 better samples\n",
      "Episode 645\tAverage Score = -5.21, sigma = 0.002.    42 better samples\n",
      "Episode 646\tAverage Score = -5.21, sigma = 0.002.    35 better samples\n",
      "Episode 647\tAverage Score = -5.14, sigma = 0.002.     4 better samples\n",
      "Episode 648\tAverage Score = -5.20, sigma = 0.002.    30 better samples\n",
      "Episode 649\tAverage Score = -5.22, sigma = 0.002.    15 better samples\n",
      "Episode 650\tAverage Score = -5.23, sigma = 0.002.    27 better samples\n",
      "Episode 651\tAverage Score = -5.20, sigma = 0.002.    10 better samples\n",
      "Episode 652\tAverage Score = -5.21, sigma = 0.002.    23 better samples\n",
      "Episode 653\tAverage Score = -5.19, sigma = 0.001.     3 better samples\n",
      "Episode 654\tAverage Score = -5.21, sigma = 0.001.    43 better samples\n",
      "Episode 655\tAverage Score = -5.29, sigma = 0.001.    42 better samples\n",
      "Episode 656\tAverage Score = -5.33, sigma = 0.001.    30 better samples\n",
      "Episode 657\tAverage Score = -5.31, sigma = 0.001.     4 better samples\n",
      "Episode 658\tAverage Score = -5.30, sigma = 0.001.    34 better samples\n",
      "Episode 659\tAverage Score = -5.29, sigma = 0.001.    46 better samples\n",
      "Episode 660\tAverage Score = -5.30, sigma = 0.001.    37 better samples\n",
      "Episode 661\tAverage Score = -5.32, sigma = 0.001.    14 better samples\n",
      "Episode 662\tAverage Score = -5.31, sigma = 0.001.    32 better samples\n",
      "Episode 663\tAverage Score = -5.39, sigma = 0.001.    45 better samples\n",
      "Episode 664\tAverage Score = -5.38, sigma = 0.001.    20 better samples\n",
      "Episode 665\tAverage Score = -5.29, sigma = 0.001.     2 better samples\n",
      "Episode 666\tAverage Score = -5.26, sigma = 0.001.     2 better samples\n",
      "Episode 667\tAverage Score = -5.20, sigma = 0.001.    20 better samples\n",
      "Episode 668\tAverage Score = -5.27, sigma = 0.001.    50 better samples\n",
      "Episode 669\tAverage Score = -5.29, sigma = 0.001.    39 better samples\n",
      "Episode 670\tAverage Score = -5.28, sigma = 0.001.     1 better samples\n",
      "Episode 671\tAverage Score = -5.25, sigma = 0.001.    21 better samples\n",
      "Episode 672\tAverage Score = -5.21, sigma = 0.001.    14 better samples\n",
      "Episode 673\tAverage Score = -5.24, sigma = 0.001.    47 better samples\n",
      "Episode 674\tAverage Score = -5.30, sigma = 0.001.    38 better samples\n",
      "Episode 675\tAverage Score = -5.21, sigma = 0.001.     3 better samples\n",
      "Episode 676\tAverage Score = -5.23, sigma = 0.001.    46 better samples\n",
      "Episode 677\tAverage Score = -5.29, sigma = 0.001.    38 better samples\n",
      "Episode 678\tAverage Score = -5.21, sigma = 0.001.     1 better samples\n",
      "Episode 679\tAverage Score = -5.23, sigma = 0.001.    15 better samples\n",
      "Episode 680\tAverage Score = -5.26, sigma = 0.001.    29 better samples\n",
      "Episode 681\tAverage Score = -5.30, sigma = 0.001.    35 better samples\n",
      "Episode 682\tAverage Score = -5.27, sigma = 0.001.    20 better samples\n",
      "Episode 683\tAverage Score = -5.23, sigma = 0.001.    24 better samples\n",
      "Episode 684\tAverage Score = -5.36, sigma = 0.001.    49 better samples\n",
      "Episode 685\tAverage Score = -5.37, sigma = 0.001.    43 better samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 686\tAverage Score = -5.34, sigma = 0.001.    42 better samples\n",
      "Episode 687\tAverage Score = -5.41, sigma = 0.001.    39 better samples\n",
      "Episode 688\tAverage Score = -5.37, sigma = 0.001.    37 better samples\n",
      "Episode 689\tAverage Score = -5.41, sigma = 0.001.    22 better samples\n",
      "Episode 690\tAverage Score = -5.45, sigma = 0.001.    46 better samples\n",
      "Episode 691\tAverage Score = -5.47, sigma = 0.001.    12 better samples\n",
      "Episode 692\tAverage Score = -5.43, sigma = 0.001.    34 better samples\n",
      "Episode 693\tAverage Score = -5.45, sigma = 0.001.    39 better samples\n",
      "Episode 694\tAverage Score = -5.39, sigma = 0.001.     1 better samples\n",
      "Episode 695\tAverage Score = -5.40, sigma = 0.001.    18 better samples\n",
      "Episode 696\tAverage Score = -5.50, sigma = 0.001.    43 better samples\n",
      "Episode 697\tAverage Score = -5.46, sigma = 0.001.    19 better samples\n",
      "Episode 698\tAverage Score = -5.42, sigma = 0.001.     1 better samples\n",
      "Episode 699\tAverage Score = -5.39, sigma = 0.001.    36 better samples\n",
      "Episode 700\tAverage Score = -5.39, sigma = 0.001.    21 better samples\n",
      "Episode 701\tAverage Score = -5.36, sigma = 0.001.     9 better samples\n",
      "Episode 702\tAverage Score = -5.32, sigma = 0.001.    13 better samples\n",
      "Episode 703\tAverage Score = -5.37, sigma = 0.001.    23 better samples\n",
      "Episode 704\tAverage Score = -5.41, sigma = 0.001.    26 better samples\n",
      "Episode 705\tAverage Score = -5.43, sigma = 0.001.    31 better samples\n",
      "Episode 706\tAverage Score = -5.46, sigma = 0.001.    35 better samples\n",
      "Episode 707\tAverage Score = -5.51, sigma = 0.001.    48 better samples\n",
      "Episode 708\tAverage Score = -5.51, sigma = 0.001.    42 better samples\n",
      "Episode 709\tAverage Score = -5.53, sigma = 0.001.    12 better samples\n",
      "Episode 710\tAverage Score = -5.59, sigma = 0.001.    37 better samples\n",
      "Episode 711\tAverage Score = -5.64, sigma = 0.001.    39 better samples\n",
      "Episode 712\tAverage Score = -5.73, sigma = 0.001.    46 better samples\n",
      "Episode 713\tAverage Score = -5.73, sigma = 0.001.    24 better samples\n",
      "Episode 714\tAverage Score = -5.69, sigma = 0.001.    42 better samples\n",
      "Episode 715\tAverage Score = -5.73, sigma = 0.001.    32 better samples\n",
      "Episode 716\tAverage Score = -5.72, sigma = 0.001.    29 better samples\n",
      "Episode 717\tAverage Score = -5.74, sigma = 0.001.    35 better samples\n",
      "Episode 718\tAverage Score = -5.74, sigma = 0.001.    32 better samples\n",
      "Episode 719\tAverage Score = -5.79, sigma = 0.001.    41 better samples\n",
      "Episode 720\tAverage Score = -5.82, sigma = 0.001.    42 better samples\n",
      "Episode 721\tAverage Score = -5.75, sigma = 0.001.     4 better samples\n",
      "Episode 722\tAverage Score = -5.75, sigma = 0.001.     5 better samples\n",
      "Episode 723\tAverage Score = -5.71, sigma = 0.001.    39 better samples\n",
      "Episode 724\tAverage Score = -5.66, sigma = 0.001.    37 better samples\n",
      "Episode 725\tAverage Score = -5.72, sigma = 0.001.    38 better samples\n",
      "Episode 726\tAverage Score = -5.63, sigma = 0.001.    11 better samples\n",
      "Episode 727\tAverage Score = -5.67, sigma = 0.001.    22 better samples\n",
      "Episode 728\tAverage Score = -5.67, sigma = 0.001.    36 better samples\n",
      "Episode 729\tAverage Score = -5.69, sigma = 0.001.    43 better samples\n",
      "Episode 730\tAverage Score = -5.63, sigma = 0.001.    24 better samples\n",
      "Episode 731\tAverage Score = -5.59, sigma = 0.001.     3 better samples\n",
      "Episode 732\tAverage Score = -5.61, sigma = 0.001.    32 better samples\n",
      "Episode 733\tAverage Score = -5.62, sigma = 0.001.    26 better samples\n",
      "Episode 734\tAverage Score = -5.65, sigma = 0.001.    38 better samples\n",
      "Episode 735\tAverage Score = -5.75, sigma = 0.001.    45 better samples\n",
      "Episode 736\tAverage Score = -5.74, sigma = 0.001.    23 better samples\n",
      "Episode 737\tAverage Score = -5.81, sigma = 0.001.    43 better samples\n",
      "Episode 738\tAverage Score = -5.84, sigma = 0.001.    24 better samples\n",
      "Episode 739\tAverage Score = -5.81, sigma = 0.001.     4 better samples\n",
      "Episode 740\tAverage Score = -5.84, sigma = 0.001.    44 better samples\n",
      "Episode 741\tAverage Score = -5.91, sigma = 0.001.    34 better samples\n",
      "Episode 742\tAverage Score = -5.92, sigma = 0.001.    26 better samples\n",
      "Episode 743\tAverage Score = -5.97, sigma = 0.001.    43 better samples\n",
      "Episode 744\tAverage Score = -6.00, sigma = 0.001.    40 better samples\n",
      "Episode 745\tAverage Score = -5.94, sigma = 0.001.    16 better samples\n",
      "Episode 746\tAverage Score = -5.93, sigma = 0.001.    28 better samples\n",
      "Episode 747\tAverage Score = -6.04, sigma = 0.001.    47 better samples\n",
      "Episode 748\tAverage Score = -5.99, sigma = 0.001.     8 better samples\n",
      "Episode 749\tAverage Score = -6.00, sigma = 0.001.    13 better samples\n",
      "Episode 750\tAverage Score = -6.02, sigma = 0.001.    37 better samples\n",
      "Episode 751\tAverage Score = -6.03, sigma = 0.001.    10 better samples\n",
      "Episode 752\tAverage Score = -5.99, sigma = 0.001.     8 better samples\n",
      "Episode 753\tAverage Score = -5.99, sigma = 0.001.     6 better samples\n",
      "Episode 754\tAverage Score = -5.97, sigma = 0.001.    26 better samples\n",
      "Episode 755\tAverage Score = -5.88, sigma = 0.001.     5 better samples\n",
      "Episode 756\tAverage Score = -5.82, sigma = 0.001.     1 better samples\n",
      "Episode 757\tAverage Score = -5.91, sigma = 0.001.    44 better samples\n",
      "Episode 758\tAverage Score = -5.84, sigma = 0.001.     7 better samples\n",
      "Episode 759\tAverage Score = -5.78, sigma = 0.001.    26 better samples\n",
      "Episode 760\tAverage Score = -5.81, sigma = 0.001.    45 better samples\n",
      "Episode 761\tAverage Score = -5.87, sigma = 0.001.    43 better samples\n",
      "Episode 762\tAverage Score = -5.86, sigma = 0.001.    30 better samples\n",
      "Episode 763\tAverage Score = -5.84, sigma = 0.000.    37 better samples\n",
      "Episode 764\tAverage Score = -5.85, sigma = 0.000.    18 better samples\n",
      "Episode 765\tAverage Score = -5.96, sigma = 0.000.    45 better samples\n",
      "Episode 766\tAverage Score = -6.01, sigma = 0.000.    29 better samples\n",
      "Episode 767\tAverage Score = -6.00, sigma = 0.000.    13 better samples\n",
      "Episode 768\tAverage Score = -5.91, sigma = 0.000.    21 better samples\n",
      "Episode 769\tAverage Score = -5.95, sigma = 0.000.    47 better samples\n",
      "Episode 770\tAverage Score = -6.05, sigma = 0.000.    45 better samples\n",
      "Episode 771\tAverage Score = -6.09, sigma = 0.000.    38 better samples\n",
      "Episode 772\tAverage Score = -6.12, sigma = 0.000.    32 better samples\n",
      "Episode 773\tAverage Score = -6.04, sigma = 0.000.    22 better samples\n",
      "Episode 774\tAverage Score = -5.99, sigma = 0.000.    18 better samples\n",
      "Episode 775\tAverage Score = -6.11, sigma = 0.000.    50 better samples\n",
      "Episode 776\tAverage Score = -6.01, sigma = 0.000.     1 better samples\n",
      "Episode 777\tAverage Score = -6.05, sigma = 0.000.    50 better samples\n",
      "Episode 778\tAverage Score = -6.11, sigma = 0.000.    30 better samples\n",
      "Episode 779\tAverage Score = -6.16, sigma = 0.000.    42 better samples\n",
      "Episode 780\tAverage Score = -6.18, sigma = 0.000.    43 better samples\n",
      "Episode 781\tAverage Score = -6.18, sigma = 0.000.    30 better samples\n",
      "Episode 782\tAverage Score = -6.17, sigma = 0.000.    12 better samples\n",
      "Episode 783\tAverage Score = -6.19, sigma = 0.000.    31 better samples\n",
      "Episode 784\tAverage Score = -6.14, sigma = 0.000.    39 better samples\n",
      "Episode 785\tAverage Score = -6.07, sigma = 0.000.    14 better samples\n",
      "Episode 786\tAverage Score = -5.98, sigma = 0.000.   *** No better samples found.\n",
      "Fine tuning is a minimum noise. Terminating search.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-98.52519241092766,\n",
       " -80.5967546255252,\n",
       " -99.8999999999986,\n",
       " -96.54051618297267,\n",
       " -99.89993443498258,\n",
       " -99.88538371218559,\n",
       " -99.86551326034002,\n",
       " -99.84704243614857,\n",
       " -99.15185484365989,\n",
       " -96.77669018436401,\n",
       " -99.89944496857798,\n",
       " -99.89766136254174,\n",
       " 31.079676410668668,\n",
       " 16.862489862435027,\n",
       " 75.9431636982766,\n",
       " 77.72132110502855,\n",
       " 88.98233026182473,\n",
       " 87.85679361528155,\n",
       " 76.42584428652646,\n",
       " 88.61587061531996,\n",
       " 78.02670393165982,\n",
       " -99.14364876397339,\n",
       " -99.8999999999986,\n",
       " -99.8999999999986,\n",
       " -99.8999999999986,\n",
       " -99.8999728918251,\n",
       " -99.8999999999986,\n",
       " -99.89997285605588,\n",
       " -97.36662489303578,\n",
       " -99.89999948739877,\n",
       " -99.8992625158487,\n",
       " -95.29681621600827,\n",
       " -99.8999999999986,\n",
       " -99.71355964705971,\n",
       " -90.30708989417792,\n",
       " -92.8544359860491,\n",
       " -83.58102914174768,\n",
       " -35.4987534721847,\n",
       " -7.048283864498838,\n",
       " -12.40252428948995,\n",
       " -3.058933488252052,\n",
       " -5.474946706785512,\n",
       " -5.506555522839676,\n",
       " -0.8071996871067222,\n",
       " -4.511508583841994,\n",
       " -1.949712945657982,\n",
       " -2.7770888179000695,\n",
       " -3.505334068472989,\n",
       " -7.319158614770555,\n",
       " -6.153210114056507,\n",
       " -4.638928742263364,\n",
       " -0.659019674847967,\n",
       " -4.752623420965354,\n",
       " -3.6740501189037054,\n",
       " -3.9291723520637594,\n",
       " -0.7879906467507987,\n",
       " -5.274378302859132,\n",
       " -1.558456427220203,\n",
       " -2.4267638371782456,\n",
       " -2.410383069723568,\n",
       " -1.988363806843082,\n",
       " -0.38042887078702603,\n",
       " -2.2006190619734967,\n",
       " -1.1823199899053762,\n",
       " -3.447538231312707,\n",
       " -12.424581982965645,\n",
       " -2.654435682717339,\n",
       " -12.407489689124688,\n",
       " -7.284039670634075,\n",
       " -4.821124135660676,\n",
       " -16.38563649948092,\n",
       " -2.6533493697834127,\n",
       " -8.066710138641055,\n",
       " -1.912510226735853,\n",
       " -11.328884178037038,\n",
       " -9.226486691630571,\n",
       " -3.0913412774334006,\n",
       " -10.653976364782777,\n",
       " -4.829230099019956,\n",
       " -0.7704474303915798,\n",
       " -8.437573158260522,\n",
       " -7.665119559113743,\n",
       " -4.9383692331704685,\n",
       " -2.0437208102202473,\n",
       " -11.591267559064537,\n",
       " -12.760537060556475,\n",
       " -0.030604270018892855,\n",
       " -0.6030024371160487,\n",
       " -3.172323851239114,\n",
       " -2.069836056120082,\n",
       " -4.957573949812112,\n",
       " -0.6942807420683649,\n",
       " -6.459948246959688,\n",
       " -2.5577934560784805,\n",
       " -8.621667084816874,\n",
       " -8.489157548712985,\n",
       " -5.887489693632306,\n",
       " -3.06524736058584,\n",
       " -5.954738854185655,\n",
       " -7.2919478716038775,\n",
       " -4.351297934760027,\n",
       " -12.439113933723268,\n",
       " -5.674043199995463,\n",
       " -1.9519263964184426,\n",
       " -6.5309531474469855,\n",
       " -4.923900520300288,\n",
       " -0.7784520491348017,\n",
       " -2.332882587097505,\n",
       " -5.460730740602225,\n",
       " -2.7645197020541277,\n",
       " -8.41329411216237,\n",
       " -2.7073689492112534,\n",
       " -9.932362204242862,\n",
       " -0.65375901253691,\n",
       " -6.449060264074429,\n",
       " -0.03079180361741727,\n",
       " -1.4400960074060745,\n",
       " -2.633608147281647,\n",
       " -7.283041224408196,\n",
       " -0.7020149176913988,\n",
       " -8.773408761594983,\n",
       " -4.161791177564194,\n",
       " -3.501558326581066,\n",
       " -0.3056382334698102,\n",
       " -2.4800445319976245,\n",
       " -1.029081206288316,\n",
       " -2.503571795861303,\n",
       " -3.696095403453072,\n",
       " -3.3549290046955793,\n",
       " -2.994272793517919,\n",
       " -7.336782457871282,\n",
       " -6.558060339706573,\n",
       " -3.948640220238632,\n",
       " -3.15030991139994,\n",
       " -3.100688894824544,\n",
       " -0.22033165197757287,\n",
       " -1.4897252806958474,\n",
       " -4.054257471049125,\n",
       " -2.8979325606116664,\n",
       " -1.6251094350737787,\n",
       " -2.8596301463267206,\n",
       " -0.4048528901038911,\n",
       " -0.32074888202312407,\n",
       " -3.9338389172409784,\n",
       " -2.1925395175592874,\n",
       " -1.7562828732141762,\n",
       " -6.140492931114359,\n",
       " -4.310204972039165,\n",
       " -9.730556691570381,\n",
       " -4.978092126958461,\n",
       " -4.537808535633273,\n",
       " -3.267727609851642,\n",
       " -1.2902908641066344,\n",
       " -1.5116390134326805,\n",
       " -2.2937775167953802,\n",
       " -1.0421076858621867,\n",
       " -6.837177299042918,\n",
       " -6.8069418287398165,\n",
       " -4.379281961615353,\n",
       " -3.409578002507352,\n",
       " -4.7869022709093185,\n",
       " -5.967808814857786,\n",
       " -6.660550461351677,\n",
       " -3.109667731532151,\n",
       " -0.9890495950248704,\n",
       " -4.636733416763252,\n",
       " -4.826108766860762,\n",
       " -4.608747668740336,\n",
       " -0.7957100426477856,\n",
       " -0.7579419469679245,\n",
       " -1.0781478512331912,\n",
       " -3.1286419811963557,\n",
       " -3.693207614156027,\n",
       " -2.662754284261052,\n",
       " -7.175553358843965,\n",
       " -2.40681502941719,\n",
       " -5.223374872206145,\n",
       " -2.9185031637778565,\n",
       " -1.4584354842254164,\n",
       " -0.4171678317794134,\n",
       " -1.7107656466247496,\n",
       " -11.62013634905981,\n",
       " -5.658567786494562,\n",
       " -6.683566051225358,\n",
       " -1.715574352885786,\n",
       " -6.334359925330672,\n",
       " -1.458531825858116,\n",
       " -8.403518959169215,\n",
       " -3.7575336237368173,\n",
       " -1.6576516060373183,\n",
       " -2.0081731672273175,\n",
       " -0.7142582513592591,\n",
       " -5.600392968149625,\n",
       " -3.2778744843247685,\n",
       " -3.845801532528203,\n",
       " -6.392855959934285,\n",
       " -2.5645267744819678,\n",
       " -0.7792490642633241,\n",
       " -3.7706254375543637,\n",
       " -1.9563524899768596,\n",
       " -4.7162609047881086,\n",
       " -4.069780859057808,\n",
       " -3.9554413814342744,\n",
       " -6.800378770226033,\n",
       " -3.3795811346349534,\n",
       " -12.162768344296103,\n",
       " -5.877824945112209,\n",
       " -6.656457927563345,\n",
       " -8.744065168465227,\n",
       " -3.133188100244893,\n",
       " -7.930551919300276,\n",
       " -3.172439937576789,\n",
       " -4.862919511663093,\n",
       " -3.8255907706857406,\n",
       " -1.9252557834594377,\n",
       " -2.0274398286848814,\n",
       " -1.594402935788113,\n",
       " -7.227116529514482,\n",
       " -10.181369184149075,\n",
       " -6.84805014970022,\n",
       " -2.557967214945964,\n",
       " -1.3490453774596503,\n",
       " -3.6860899125063495,\n",
       " -4.917350237633922,\n",
       " -7.1799432228902935,\n",
       " -1.1879154779733487,\n",
       " -9.867797637296153,\n",
       " -0.5788134410928595,\n",
       " -7.028979775664833,\n",
       " -0.4587573481564765,\n",
       " -4.189690940760682,\n",
       " -5.063458404646186,\n",
       " -5.527881477945033,\n",
       " -9.275344980669388,\n",
       " -2.3487859597047605,\n",
       " -1.0908696702208713,\n",
       " -5.663128207147915,\n",
       " -4.645504540665973,\n",
       " -4.6846667474967685,\n",
       " -3.5977282996830784,\n",
       " -6.636630274159123,\n",
       " -3.7237821686946795,\n",
       " -1.3604972479838344,\n",
       " -1.3645797830081874,\n",
       " -1.6504883123373693,\n",
       " -2.174932570403677,\n",
       " -4.946573349786705,\n",
       " -3.2979165017824568,\n",
       " -5.381062584685622,\n",
       " -1.738327528134269,\n",
       " -0.5410402106458951,\n",
       " -6.030368364434021,\n",
       " -8.78969713526543,\n",
       " -12.151265770354591,\n",
       " -2.92845976009582,\n",
       " -5.01371772451286,\n",
       " -2.338164610959123,\n",
       " -2.346493067210858,\n",
       " -0.49886894183124025,\n",
       " -0.2162383803303512,\n",
       " -0.43907863129798147,\n",
       " -0.4994443307120448,\n",
       " -11.037926351074924,\n",
       " -0.2732716028783883,\n",
       " -6.37862546059682,\n",
       " -2.638569279987568,\n",
       " -3.5555736575195724,\n",
       " -13.403857978559463,\n",
       " -3.518357767303579,\n",
       " -2.302563984882779,\n",
       " -10.405638706083849,\n",
       " -3.6218549600314076,\n",
       " -5.626670308642558,\n",
       " -0.8022689155173409,\n",
       " -2.9985558643884134,\n",
       " -9.622814198072891,\n",
       " -10.45973857654032,\n",
       " -4.81720224888909,\n",
       " -0.3854586759011307,\n",
       " -5.273113516214496,\n",
       " -1.5635279577888417,\n",
       " -8.34987409077359,\n",
       " -3.853463536557382,\n",
       " -6.899319083698204,\n",
       " -4.515264175639814,\n",
       " -0.7188235341518995,\n",
       " -2.459749760002372,\n",
       " -7.266034355953817,\n",
       " -5.479419975649138,\n",
       " -7.120728927888012,\n",
       " -2.7104682896037686,\n",
       " -10.747139612837689,\n",
       " -3.49420650646438,\n",
       " -7.985321493452658,\n",
       " -1.8028756822660181,\n",
       " -3.934608854411645,\n",
       " -3.7323428085409502,\n",
       " -4.438151715215194,\n",
       " -4.499830678740233,\n",
       " -6.853643341516998,\n",
       " -6.985864740547049,\n",
       " -4.797103889137529,\n",
       " -5.092874784153801,\n",
       " -7.450829819032442,\n",
       " -5.643298877876462,\n",
       " -6.300973635991115,\n",
       " -4.975652858898993,\n",
       " -9.60563236900605,\n",
       " -8.971315672970674,\n",
       " -5.752657905109424,\n",
       " -2.1047862718300347,\n",
       " -11.098427108778077,\n",
       " -11.652627819086915,\n",
       " -6.949247681530864,\n",
       " -13.795719548589503,\n",
       " -5.778974022851833,\n",
       " -2.5228097222089825,\n",
       " -14.575282037897246,\n",
       " -7.733653151734034,\n",
       " -5.4187727855340535,\n",
       " -5.436130223459311,\n",
       " -7.275930345006277,\n",
       " -9.167571703604931,\n",
       " -6.7981666821621625,\n",
       " -14.224739953431955,\n",
       " -1.296777612191282,\n",
       " -0.9811929238595423,\n",
       " -7.296013327677403,\n",
       " -3.642496699542762,\n",
       " -10.465760575793968,\n",
       " -2.708401488548376,\n",
       " -5.240704389128855,\n",
       " -1.2495137483497483,\n",
       " -3.380616969744833,\n",
       " -7.437173157761776,\n",
       " -4.349784348871487,\n",
       " -3.7995885091384913,\n",
       " -2.424488106631009,\n",
       " -7.479459649619861,\n",
       " -5.566969907204078,\n",
       " -9.735375192817987,\n",
       " -4.897974338928725,\n",
       " -9.20602101350896,\n",
       " -7.83113092468109,\n",
       " -2.634018662792729,\n",
       " -6.844159859559786,\n",
       " -6.838000584322455,\n",
       " -11.20864638076588,\n",
       " -6.912846511182378,\n",
       " -5.163293721040551,\n",
       " -10.19063839447995,\n",
       " -5.131803742275775,\n",
       " -5.718098477239656,\n",
       " -6.581073924237468,\n",
       " -1.321284674297475,\n",
       " -2.215113117064128,\n",
       " -10.131667516778077,\n",
       " -7.9308743752742314,\n",
       " -3.3815259356714846,\n",
       " -1.540131073515414,\n",
       " -5.098321519818735,\n",
       " -9.622322753371291,\n",
       " -9.813374961532269,\n",
       " -10.49127500163884,\n",
       " -7.030639327691039,\n",
       " -5.454063952773984,\n",
       " -5.399936847172941,\n",
       " -5.1454794722329265,\n",
       " -4.121213360701835,\n",
       " -9.756343677928042,\n",
       " -2.1983620880593353,\n",
       " -6.047315190609701,\n",
       " -5.272810646403131,\n",
       " -5.151585200126034,\n",
       " -1.0805887152586553,\n",
       " -5.276844600702855,\n",
       " -5.320392228712774,\n",
       " -7.098495577463411,\n",
       " -6.425697878883797,\n",
       " -7.423429246619955,\n",
       " -3.77266062402258,\n",
       " -8.685410291931397,\n",
       " -3.63963467384054,\n",
       " -7.659455703504093,\n",
       " -13.417324846236077,\n",
       " -0.6395638407096347,\n",
       " -5.402774209222755,\n",
       " -9.122992758021935,\n",
       " -3.581409769574035,\n",
       " -1.9793361666954807,\n",
       " -5.114494890244797,\n",
       " -8.54132629106851,\n",
       " -0.569857088125511,\n",
       " -12.520698610107061,\n",
       " -1.1742929830558009,\n",
       " -3.4751227666029187,\n",
       " -4.1677903272840116,\n",
       " -6.689043194949025,\n",
       " -2.0782598659681533,\n",
       " -6.613480464138745,\n",
       " -6.265143962613463,\n",
       " -8.895087486971834,\n",
       " -3.746645622834864,\n",
       " -2.3732814043772565,\n",
       " -4.573326588175145,\n",
       " -1.3108390919903754,\n",
       " -5.061949842510833,\n",
       " -7.034833606802212,\n",
       " -9.845922014343063,\n",
       " -6.661350651747648,\n",
       " -1.2804797093021838,\n",
       " -11.799960540672632,\n",
       " -9.159353054289555,\n",
       " -6.8021684679741155,\n",
       " -7.886570681726463,\n",
       " -11.468688797536133,\n",
       " -5.042550824836634,\n",
       " -5.016748797395942,\n",
       " -7.304116327418267,\n",
       " -4.773070263478767,\n",
       " -6.303736103936184,\n",
       " -6.799792836055118,\n",
       " -7.980524610539444,\n",
       " -3.6380597343166903,\n",
       " -1.743645044682212,\n",
       " -8.879120939079423,\n",
       " -7.206855574354536,\n",
       " -0.5557186137930142,\n",
       " -7.403679594895105,\n",
       " -4.214637065091751,\n",
       " -7.176847424237678,\n",
       " -11.188745022686302,\n",
       " -4.143836987365203,\n",
       " -6.024477355992587,\n",
       " -2.8853049100297925,\n",
       " -3.6840524316609846,\n",
       " -0.20651084833700048,\n",
       " -3.3960592139999037,\n",
       " -5.684794221205509,\n",
       " -0.23987336783838542,\n",
       " -4.986897979078942,\n",
       " -2.5484686655899838,\n",
       " -0.7723989144035247,\n",
       " -2.045813263312406,\n",
       " -9.044496828659812,\n",
       " -5.006718093909054,\n",
       " -0.27352602199799286,\n",
       " -11.029895755525748,\n",
       " -5.661746934498061,\n",
       " -8.71532366185252,\n",
       " -2.8552886054457143,\n",
       " -2.363276288882256,\n",
       " -9.66676044545561,\n",
       " -7.116029768240505,\n",
       " -5.833801727398916,\n",
       " -4.7742708128600535,\n",
       " -2.908824064999256,\n",
       " -10.430020378232188,\n",
       " -9.651521365761278,\n",
       " -2.5945140401597437,\n",
       " -3.6841795964382067,\n",
       " -7.867974890532168,\n",
       " -7.991470787871508,\n",
       " -8.822346099359038,\n",
       " -6.613911430912937,\n",
       " -11.995428372812887,\n",
       " -0.4590482417780936,\n",
       " -6.381460172484575,\n",
       " -0.469283819193313,\n",
       " -5.086641918813296,\n",
       " -8.405075934088597,\n",
       " -2.755095712314082,\n",
       " -0.5365781521446822,\n",
       " -4.988926423380587,\n",
       " -5.585470233179663,\n",
       " -8.004692814569973,\n",
       " -10.674123355065012,\n",
       " -2.4567774847524824,\n",
       " -5.45649717783282,\n",
       " -5.7074748245709275,\n",
       " -3.6385870452411835,\n",
       " -7.819114358281438,\n",
       " -6.255695278466273,\n",
       " -0.4989582851141847,\n",
       " -0.7173099670188566,\n",
       " -0.6201914095281936,\n",
       " -0.6218521782677631,\n",
       " -0.752109360045017,\n",
       " -2.501797716955144,\n",
       " -12.624083579141683,\n",
       " -8.835220369893712,\n",
       " -2.368351400102017,\n",
       " -0.2492615863745883,\n",
       " -2.8168069868184387,\n",
       " -11.479403992362833,\n",
       " -7.786090820561314,\n",
       " -3.7064066457858136,\n",
       " -12.643415990518124,\n",
       " -2.1891980060346246,\n",
       " -1.0630712311646873,\n",
       " -2.7668051793212505,\n",
       " -5.650344767204759,\n",
       " -11.478085242147003,\n",
       " -0.37281543899734026,\n",
       " -2.816158230257719,\n",
       " -5.592415343303134,\n",
       " -7.161134093673735,\n",
       " -6.966372680649608,\n",
       " -12.217529331674676,\n",
       " -4.9793446315049925,\n",
       " -1.3601110359670805,\n",
       " -5.71776733452683,\n",
       " -4.599527781826104,\n",
       " -9.324567162445458,\n",
       " -8.146561801053265,\n",
       " -6.592984440244665,\n",
       " -9.599112471412521,\n",
       " -0.6733544639351696,\n",
       " -2.5701681454335215,\n",
       " -8.999812655978475,\n",
       " -12.1392112938813,\n",
       " -2.3103187643126337,\n",
       " -0.4288304562199723,\n",
       " -1.454942062947014,\n",
       " -3.413025674018666,\n",
       " -7.085162359149166,\n",
       " -8.001051726236113,\n",
       " -6.932299307005832,\n",
       " -5.056465068507227,\n",
       " -7.013258961101929,\n",
       " -5.774711902044863,\n",
       " -8.63708125392429,\n",
       " -11.397051440499004,\n",
       " -6.169651430980394,\n",
       " -9.664738385597104,\n",
       " -9.628416443524577,\n",
       " -2.8851771940127744,\n",
       " -4.931508646411393,\n",
       " -1.828827275277791,\n",
       " -7.432413661349507,\n",
       " -2.8817391506025754,\n",
       " -3.626614367705248,\n",
       " -2.284481580924443,\n",
       " -5.845730051413955,\n",
       " -1.7341515141872736,\n",
       " -1.10133162809897,\n",
       " -6.897726484429031,\n",
       " -6.998190586606645,\n",
       " -0.3860981910186934,\n",
       " -0.8865142603691814,\n",
       " -3.667440541261928,\n",
       " -5.690139359171219,\n",
       " -3.67297035882369,\n",
       " -3.6694068097290597,\n",
       " -6.9448564353274,\n",
       " -2.075851877667376,\n",
       " -2.5236699404120033,\n",
       " -2.365889864736918,\n",
       " -8.205815962443513,\n",
       " -11.533454608863389,\n",
       " -5.926165942506416,\n",
       " -0.9394615492910786,\n",
       " -7.777641150209332,\n",
       " -1.6688731084449349,\n",
       " -4.591333465916832,\n",
       " -9.650416786894109,\n",
       " -2.8446851808670215,\n",
       " -11.08974646901419,\n",
       " -5.631161351699148,\n",
       " -5.152632855652869,\n",
       " -2.8093908068235223,\n",
       " -7.253984507981463,\n",
       " -6.914968210032691,\n",
       " -7.76968884827529,\n",
       " -2.310423569862984,\n",
       " -9.65407685936107,\n",
       " -8.237064783835127,\n",
       " -1.8567040125527075,\n",
       " -8.786578032988503,\n",
       " -0.8930193988910491,\n",
       " -1.8670216633469565,\n",
       " -1.8832312403912097,\n",
       " -7.076276544639796,\n",
       " -8.393063361695564,\n",
       " -0.28642286324246785,\n",
       " -8.124484910414678,\n",
       " -12.326010215266246,\n",
       " -0.6028920031207441,\n",
       " -11.757572613230339,\n",
       " -1.3550449839721805,\n",
       " -6.92822203252091,\n",
       " -2.546066162216725,\n",
       " -11.715973192138014,\n",
       " -5.782163445816689,\n",
       " -6.36746809938776,\n",
       " -1.8953879514517418,\n",
       " -0.16158547577794993,\n",
       " -7.898516699020646,\n",
       " -3.9874106831013183,\n",
       " -11.371160560466254,\n",
       " -3.9633297623876476,\n",
       " -3.9424532542477078,\n",
       " -5.115527754986387,\n",
       " -0.2158034375766177,\n",
       " -1.9293268205838296,\n",
       " -3.4288694430747064,\n",
       " -4.0464179529987545,\n",
       " -7.281980447767416,\n",
       " -9.641438770632352,\n",
       " -0.5066671755460183,\n",
       " -1.6530378052474748,\n",
       " -2.853238242415623,\n",
       " -2.6699573576799116,\n",
       " -6.235621031773144,\n",
       " -11.925485338678458,\n",
       " -2.567752982091285,\n",
       " -8.320827383460594,\n",
       " -4.0821083686330715,\n",
       " -6.2542876882060785,\n",
       " -3.2058959962157116,\n",
       " -6.245500127998791,\n",
       " -8.785973895385032,\n",
       " -1.1014907602717117,\n",
       " -11.806675989632856,\n",
       " -12.621306967091053,\n",
       " -1.2225680538357186,\n",
       " -11.546134880515847,\n",
       " -0.9752805673675465,\n",
       " -7.765180456615641,\n",
       " -8.553425950862016,\n",
       " -9.454320458442005,\n",
       " -4.779409466854297,\n",
       " -4.707445673297353,\n",
       " -3.375873837094358,\n",
       " -4.155816815328671,\n",
       " -0.37015600564289036,\n",
       " -5.470784185166281,\n",
       " -0.6343539592127542,\n",
       " -3.2968781823166378,\n",
       " -3.3195872578308157,\n",
       " -7.276030539420393,\n",
       " -0.47156470760723185,\n",
       " -4.017838459745272,\n",
       " -5.7529543185959415,\n",
       " -4.971291073271753,\n",
       " -8.638831349325633,\n",
       " -7.031847390364228,\n",
       " -0.5357644817420462,\n",
       " -5.711206733408489,\n",
       " -2.8606527347095363,\n",
       " -5.402903674646672,\n",
       " -1.9526505384066568,\n",
       " -5.118788162219992,\n",
       " -1.337106667307448,\n",
       " -8.683248953881442,\n",
       " -10.773435139410442,\n",
       " -5.840573082747216,\n",
       " -0.4893562923922543,\n",
       " -7.454908732413092,\n",
       " -10.747810738447122,\n",
       " -7.07656584152881,\n",
       " -2.5342773793210016,\n",
       " -6.561748768957194,\n",
       " -9.734051307827663,\n",
       " -3.672602406805372,\n",
       " -0.6153921882967711,\n",
       " -0.465686911571652,\n",
       " -4.20664086659957,\n",
       " -12.624469441465951,\n",
       " -7.983256798743148,\n",
       " -1.105978569805286,\n",
       " -4.143192800795566,\n",
       " -2.8798275520460894,\n",
       " -11.406074897218696,\n",
       " -8.167334167689972,\n",
       " -0.6839036019539391,\n",
       " -10.191647727054331,\n",
       " -8.25908954920834,\n",
       " -0.6568736561466612,\n",
       " -2.5198677404721086,\n",
       " -4.99611158764045,\n",
       " -5.725185818256403,\n",
       " -4.406409412034469,\n",
       " -4.636697282578315,\n",
       " -12.3828201728577,\n",
       " -9.712358868898365,\n",
       " -8.854962398260973,\n",
       " -8.095512131306569,\n",
       " -7.441608748780846,\n",
       " -5.447118221391315,\n",
       " -11.461981592061663,\n",
       " -3.644472938687581,\n",
       " -8.093568866853222,\n",
       " -7.780426830415575,\n",
       " -0.4966954275423711,\n",
       " -2.8918206978420606,\n",
       " -10.341521201541013,\n",
       " -3.4386883829817947,\n",
       " -0.6575717093821878,\n",
       " -7.818447283186639,\n",
       " -3.6608010690452475,\n",
       " -0.9885437332702544,\n",
       " -1.9294156302031444,\n",
       " -5.16327047320393,\n",
       " -5.7091520690154915,\n",
       " -5.660210908167642,\n",
       " -7.018273654962263,\n",
       " -11.695460603582958,\n",
       " -9.700032505937019,\n",
       " -2.6436957127062497,\n",
       " -7.7260346194614495,\n",
       " -7.989709921390981,\n",
       " -11.546728604561702,\n",
       " -5.6572836180743655,\n",
       " -8.036749922933485,\n",
       " -6.995330747457643,\n",
       " -6.912368108817763,\n",
       " -6.197129126260944,\n",
       " -6.274014248053946,\n",
       " -8.87712457687912,\n",
       " -8.977063665347098,\n",
       " -1.3366154458939634,\n",
       " -1.3593348858935748,\n",
       " -8.260278120151307,\n",
       " -7.0554217595291995,\n",
       " -6.948428204044598,\n",
       " -2.5628711266298847,\n",
       " -5.699589136384412,\n",
       " -7.712401792274253,\n",
       " -10.346963904799184,\n",
       " -3.6264307760655115,\n",
       " -0.616140184111495,\n",
       " -6.224599913515212,\n",
       " -4.9004665838684796,\n",
       " -6.935380260268324,\n",
       " -10.705487042041739,\n",
       " -4.113808767826146,\n",
       " -7.92290285180012,\n",
       " -5.699941133323668,\n",
       " -0.8799920173530068,\n",
       " -9.732101941159693,\n",
       " -7.828550464403268,\n",
       " -5.451046110923314,\n",
       " -10.587030212077705,\n",
       " -7.799922944237308,\n",
       " -2.8110764577748037,\n",
       " -5.695953941413911,\n",
       " -11.533401729782803,\n",
       " -1.1848474176658734,\n",
       " -3.1400366421338735,\n",
       " -8.166824881435252,\n",
       " -2.777757296233697,\n",
       " -1.220947385225236,\n",
       " -1.3347687345792985,\n",
       " -6.226117005962326,\n",
       " -1.8910024129348477,\n",
       " -0.23230301526851388,\n",
       " -8.73594957124101,\n",
       " -1.2535739940748103,\n",
       " -4.151132563516545,\n",
       " -10.588699190752498,\n",
       " -7.7358492700124675,\n",
       " -6.184218132656253,\n",
       " -7.185182208246457,\n",
       " -4.562899612562894,\n",
       " -11.739376823995237,\n",
       " -5.693660226897185,\n",
       " -2.8702930496734504,\n",
       " -4.236765412547063,\n",
       " -11.807042965472718,\n",
       " -10.854770494467777,\n",
       " -8.220063844794929,\n",
       " -5.67896355297272,\n",
       " -4.188170311061728,\n",
       " -2.58916670468845,\n",
       " -12.60828252788466,\n",
       " -0.27745724351359463,\n",
       " -12.704640556841909,\n",
       " -6.243000884967679,\n",
       " -7.798526556061421,\n",
       " -7.255105769525189,\n",
       " -5.467735892013746,\n",
       " -3.6421360797841342,\n",
       " -5.810786159123833,\n",
       " -8.242983374048698,\n",
       " -2.048700330936075,\n",
       " -0.3996445434063901]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 101\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "agent = Agent(env).to(device)\n",
    "\n",
    "scores = train(agent, init_sigma=0.5, num_samples=50, print_every=1, winning_score=90.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
