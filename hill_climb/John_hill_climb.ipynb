{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Continuous - Hill Climbing Practice\n",
    "\n",
    "This notebook experiments with techniques from the Udacity DRL course for hill climbing solutions to finding an optimum policy.  Code is modified from that provided in the CEM notebook provided in class.  The goal is to solve the OpenAI Gym's mountain car environment with continuous control inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvirtualdisplay\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/05/6568620fed440941b704664b9cfe5f836ad699ac7694745e7787fbdc8063/PyVirtualDisplay-2.0-py2.py3-none-any.whl\n",
      "Collecting EasyProcess (from pyvirtualdisplay)\n",
      "  Using cached https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
      "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
      "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "!python -m pip install pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the MountanCarContinuous environment & detect computing platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    \n",
    "    # Takes in the game environment so that it can size the NN layers to match states (inputs)\n",
    "    # and actions (outputs).\n",
    "    # env:  the game environment (assumes OpenAI Gym API)\n",
    "    # h_size:  the number of neurons in the hidden layer\n",
    "    \n",
    "    def __init__(self, env, h_size=16):\n",
    "        super(Agent, self).__init__()\n",
    "        self.env = env\n",
    "        \n",
    "        # state, hidden layer, action sizes\n",
    "        self.s_size = env.observation_space.shape[0]\n",
    "        self.h_size = h_size\n",
    "        self.a_size = env.action_space.shape[0]\n",
    "        \n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.s_size, self.h_size)\n",
    "        self.fc2 = nn.Linear(self.h_size, self.a_size)\n",
    "        \n",
    "        \n",
    "    # Unmarshals and stores the weights & biases in preparation for computation\n",
    "    # weights:  a list of all the weights & biases in the model; the first part of the list\n",
    "    #           is all the fc1 weights, then all the fc1 biases, then all the fc2 weights\n",
    "    #           and finally all the fc2 biases\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        s = self.s_size\n",
    "        h = self.h_size\n",
    "        a = self.a_size\n",
    "        \n",
    "        # separate the weights & biases for each layer\n",
    "        fc1_end = s*h + h\n",
    "        fc1_W = torch.from_numpy(weights[:s*h].reshape(s, h))\n",
    "        fc1_b = torch.from_numpy(weights[s*h:fc1_end])\n",
    "        fc2_W = torch.from_numpy(weights[fc1_end:fc1_end+(h*a)].reshape(h, a))\n",
    "        fc2_b = torch.from_numpy(weights[fc1_end+(h*a):])\n",
    "        \n",
    "        # set the weights for each layer\n",
    "        self.fc1.weight.data.copy_(fc1_W.view_as(self.fc1.weight.data))\n",
    "        self.fc1.bias.data.copy_(  fc1_b.view_as(self.fc1.bias.data))\n",
    "        \n",
    "        self.fc2.weight.data.copy_(fc2_W.view_as(self.fc2.weight.data))\n",
    "        self.fc2.bias.data.copy_(  fc2_b.view_as(self.fc2.bias.data))\n",
    "    \n",
    "    \n",
    "    # Returns the length of the marshalled weights list\n",
    "    def get_weights_dim(self):\n",
    "        return (self.s_size+1)*self.h_size + (self.h_size+1)*self.a_size\n",
    "    \n",
    "    \n",
    "    # Performs the forward pass computation of the NN\n",
    "    # x:  the vector of input data (environment states)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        return x.cpu().data\n",
    "    \n",
    "    \n",
    "    # Plays one episode of the game in order to evaluate the return generated by the given policy.\n",
    "    # The NN embodies the policy definition, therefore its weights encode this policy.\n",
    "    # weights:  a list of the NN's weights & biases\n",
    "    # gamma:  the discount factor\n",
    "    # max_t:  max number of time steps allowed in an episode\n",
    "    \n",
    "    def evaluate(self, weights, gamma=1.0, max_t=5000):\n",
    "        self.set_weights(weights)\n",
    "        episode_return = 0.0\n",
    "        state = self.env.reset()\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            action = self.forward(state)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            episode_return += reward * math.pow(gamma, t)\n",
    "            if done:\n",
    "                break\n",
    "        return episode_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Each of the hill climbing methods begins with a baseline policy, then samples one or more random perturbations around that baseline.  The difference is in how those samples are used to create a new baseline for the next learning iteration.\n",
    "\n",
    "The cross-entropy method gathers many samples, then chooses the best few of these (with the highest returns on a single episode) and averages them together to form the new baseline.\n",
    "\n",
    "The evolution method uses several samples (possibly fewer than for cross-entropy), then uses a weighted average of all of them to form the new baseline, the weighting being proportional to the return from each sample.  Thus, the samaples \"higher up the hill\" have more influence in moving the baseline.\n",
    "\n",
    "In my view, cross-entropy is a generalized improvement over the evolution method; by throwing away the lower performing samples it is effectively assigning them a weight of zero.  The down-side is that by averaging the highest performing samples, it is ignoring the differences in their returns, thus possibly watering down the potential gradient ascent.  Therefore, the approach below is a hybrid of the two:  throw out any samples with reward lower than that achieved by the baseline, then use a weighted average of the remaining samples, if any.\n",
    "\n",
    "In addition to this more aggressive hill climbing, the code uses two more techniques to help avoid getting stuck at a local maximum.\n",
    "1. Using multiple random starting points within the state space\n",
    "2. Adaptive noise scaling, that changes the size of the sample perturbations based on recent level of success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# agent:       the agent model to be trained\n",
    "# max_epochs:  max number of epochs to train\n",
    "# winning_score: the game score above which the agent is considered to be adequately trained\n",
    "# gamma:       time step discount factor\n",
    "# print_every: number of epochs between status reporting\n",
    "# num_samples: number of randomly perturbed samples to be evaluated around the baseline\n",
    "# init_sigma:  the initial standard deviation of the sample perturbations\n",
    "# Return:  list of scores of the baseline model from each epoch\n",
    "\n",
    "def train(agent, max_epochs=1000, winning_score=100.0, gamma=1.0, print_every=10, \n",
    "          num_samples=10, init_sigma=0.5):\n",
    "\n",
    "    # store the most recent 100 epoch returns\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    \n",
    "    # get initial random weights & biases for the model and loop on epochs\n",
    "    sigma = init_sigma\n",
    "    min_sigma = init_sigma\n",
    "    weight_size = agent.get_weights_dim()\n",
    "    baseline = sigma*np.random.randn(weight_size)\n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        # evaluate the baseline and store its return\n",
    "        reward = agent.evaluate(baseline, gamma=gamma)\n",
    "        scores_deque.append(reward)\n",
    "        scores.append(reward)\n",
    "        print(\"Baseline reward = \", reward)\n",
    "        \n",
    "        # print status\n",
    "        if epoch % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(epoch, np.mean(scores_deque)))\n",
    "\n",
    "        # if average scores are high enough, end the training\n",
    "        if np.mean(scores_deque) >= winning_score:\n",
    "            print('\\nEnvironment solved in {:d} epochs!\\tAvg Score: {:.2f}'.format(epoch, \n",
    "                                                                                   np.mean(scores_deque)))\n",
    "            break\n",
    "\n",
    "        # generate random samples around the baseline\n",
    "        samples = [baseline + sigma*np.random.randn(weight_size) for i in range(num_samples)]\n",
    "        \n",
    "        # evaluate each sample and store its return if it is better than the baseline return\n",
    "        sample_rewards = []\n",
    "        sample_weights = []\n",
    "        for s in range(num_samples):\n",
    "            r = agent.evaluate(samples[s], gamma=gamma)\n",
    "            print(\"Sample {} reward = {:.3f}\".format(s, r))\n",
    "            if r > reward:\n",
    "                sample_rewards.append(r)\n",
    "                sample_weights.append(samples[s])\n",
    "        \n",
    "        # if at least one sample performed better than the baseline then\n",
    "        if len(sample_rewards) > 0:\n",
    "            print(\"Found {} samples better than the baseline\".format(len(sample_rewards)))\n",
    "        \n",
    "            # get the weighted average of all better samples and consider this the new baseline\n",
    "            \n",
    "            # reduce the noise magnitude and store it as the new minimum noise\n",
    "            \n",
    "        # else (no samples performed as well as the baseline; we may have found the top of the hill)\n",
    "        else:\n",
    "            print(\"No better samples found.\")\n",
    "        \n",
    "            # if we are still in adaptive noise phase then\n",
    "        \n",
    "                # increment the counter of no improvement\n",
    "            \n",
    "                # if we haven't seen improvement for several epochs then\n",
    "                \n",
    "                    # set noise to min used thus far and indicate transition to fine tuning phase\n",
    "                    # since it appears we are close to the global maximum\n",
    "\n",
    "                # increase the noise magnitude\n",
    "                \n",
    "            # else (in fine tuning phase)\n",
    "            \n",
    "                # reduce the noise\n",
    "                \n",
    "                # increment the final phase counter and terminate after enough with no improvement\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline reward =  -53.72886239719742\n",
      "Episode 0\tAverage Score: -53.73\n",
      "Sample 0 reward = -99.246\n",
      "Sample 1 reward = -41.204\n",
      "Sample 2 reward = -29.902\n",
      "Sample 3 reward = -83.281\n",
      "Sample 4 reward = -93.246\n",
      "Sample 5 reward = -96.460\n",
      "Sample 6 reward = -7.604\n",
      "Sample 7 reward = -99.011\n",
      "Sample 8 reward = -73.191\n",
      "Sample 9 reward = -18.530\n",
      "Found 4 samples better than the baseline\n",
      "Baseline reward =  -53.68761720014443\n",
      "Sample 0 reward = -85.806\n",
      "Sample 1 reward = -75.100\n",
      "Sample 2 reward = -98.815\n",
      "Sample 3 reward = -74.443\n",
      "Sample 4 reward = -90.806\n",
      "Sample 5 reward = -4.071\n",
      "Sample 6 reward = -4.727\n",
      "Sample 7 reward = -11.394\n",
      "Sample 8 reward = -98.387\n",
      "Sample 9 reward = -99.662\n",
      "Found 3 samples better than the baseline\n",
      "Baseline reward =  -53.796730922058764\n",
      "Sample 0 reward = -27.778\n",
      "Sample 1 reward = -70.008\n",
      "Sample 2 reward = -87.477\n",
      "Sample 3 reward = -91.850\n",
      "Sample 4 reward = -94.230\n",
      "Sample 5 reward = -43.049\n",
      "Sample 6 reward = -96.744\n",
      "Sample 7 reward = -87.006\n",
      "Sample 8 reward = -83.737\n",
      "Sample 9 reward = -74.212\n",
      "Found 2 samples better than the baseline\n",
      "Baseline reward =  -53.323605605937324\n",
      "Sample 0 reward = -71.299\n",
      "Sample 1 reward = -0.763\n",
      "Sample 2 reward = -95.090\n",
      "Sample 3 reward = -4.343\n",
      "Sample 4 reward = -56.464\n",
      "Sample 5 reward = -51.062\n",
      "Sample 6 reward = -99.423\n",
      "Sample 7 reward = -99.822\n",
      "Sample 8 reward = -95.187\n",
      "Sample 9 reward = -77.246\n",
      "Found 3 samples better than the baseline\n",
      "Baseline reward =  -53.8517598759685\n",
      "Sample 0 reward = -34.300\n",
      "Sample 1 reward = -99.271\n",
      "Sample 2 reward = -99.865\n",
      "Sample 3 reward = -0.689\n",
      "Sample 4 reward = -99.826\n",
      "Sample 5 reward = -96.959\n",
      "Sample 6 reward = -64.872\n",
      "Sample 7 reward = -98.634\n",
      "Sample 8 reward = -34.785\n",
      "Sample 9 reward = -0.894\n",
      "Found 4 samples better than the baseline\n",
      "Baseline reward =  -53.61787223525959\n",
      "Sample 0 reward = -41.742\n",
      "Sample 1 reward = -1.901\n",
      "Sample 2 reward = -6.627\n",
      "Sample 3 reward = -73.786\n",
      "Sample 4 reward = -94.208\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bd53090cc776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a172695dc5a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, max_epochs, winning_score, gamma, print_every, num_samples, init_sigma)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample {} reward = {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-04c7e9f0975a>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, weights, gamma, max_t)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-04c7e9f0975a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mtanh\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m     \"\"\"\n\u001b[0;32m--> 964\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.seed(101)\n",
    "np.random.seed(101)\n",
    "agent = Agent(env).to(device)\n",
    "\n",
    "train(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
